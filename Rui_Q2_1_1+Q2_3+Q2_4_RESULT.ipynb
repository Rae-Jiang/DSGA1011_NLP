{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Rui_Q2.1.1+Q2.3+Q2.4_RESULT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rae-Jiang/DSGA1011_NLP/blob/master/Rui_Q2_1_1%2BQ2_3%2BQ2_4_RESULT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhzMpzQvDfq-",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQfThHknDB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "  \n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsO4H9MIDfq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYhzi9CC3yqJ",
        "colab_type": "code",
        "outputId": "207cd916-adcc-4cd9-ca11-08ba7806eb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "basedir = '/content/drive/My Drive/1011 NLP/HW2/Playground_Rui'\n",
        "# data_folder_path = os.path.join(basedir, 'data')\n",
        "model_folder_path = os.path.join(basedir, 'model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j73WKUf6DfrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ayhuc3-DfrJ",
        "colab_type": "code",
        "outputId": "47960c74-83c7-4e55-837b-4c542819319f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "# delta = 0.0005\n",
        "# for n in [2, 3, 4]:\n",
        "#     lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm.estimate(datasets['train'])\n",
        "\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-07 00:21:48--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 103.116.4.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|103.116.4.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-07 00:21:48--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-07 00:21:48--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 103.116.4.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|103.116.4.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!tLdrRqaJe6HDrmjd_xTKdlScFg8GT_u3QEYBQLOeA2zrhpeIODETJAgCteMfo-clukndliLR6HyJVywzpCV-wP1VJ0xT0T__vC6YjrHmI_COMPa5zuwL0ELE7K_SPqIiu7yq1thXecFlAgqyhO45oYblLRczEL1Yzmsp4jPmNctqs6YcibyQPs9R-cofKqE3wt7RGMzF7pxkhMnpnnIYkh4Mr-fRI4f6XuIn38oWAHnTiiwBTb5INc4mWIK1A6pWFCaaCz0hPgR3jxbP4zosR-LnbsFkT-vWsDAqX6hXgk8iOLF-ui-TcWyxzhnssdCOmcGxemnjrGL4mVrtODJWt9-8GoP6-nrEzPq5mkfXCbXZiMWasp84xTTcS0bF6fj6oX1EqL-K7SbJH6PfVEsqCdZmeGhgSJjyP7iZjCButoiDm9waiFPJaDcBtgEsHQbOex-kTr1k6OwoWnxMb9C-AScWmbB1V2cVemXla8t1duE6AQkKg1f2m1d_1bOWZx4PXENDJ_IXizlDp9WqUr8GJ2bVPb8OeHtb6GspWPuv1Kqw4CHrc10xQo3DLI3BjbXNNzl-QdUKE3pBDTl_NABRkPpHsDmGhBlJRJLu5JddSZ1SWCEmCV_rt8xbnhOfUGgCYvJ5jsQYXsXiVdbqFgdgXeGiwy271k3NpBOb2grXmQgs3xud8IKK8N6ugAExPER89BISAeoA5FIDNkwMg-sdb4oRDgNxbFP6CqmqFVWBUTFhyNhFMyYax7TPjpY9ideGpyg4nlWw4x3KYkc1SBsUdv_6g5yuR3rOVv7xgGpAqczfzdj5rSBC3YG-DfHRytJIuc1r6nVSSqFBgZlGrVuLwgjIo_HT2oa5YWP8ZrMt3SJb1dHy0tYhMCc7I-803g1X2WXwab5D5tIM8f0ckqSyAzKhwc_G7ys-WAAf1U_ghE_BQ39-NRTDhoBx7v9lb6bLbZM4Vo3CEx-ZZIXJ5ZHQEr16VXFEfjqwKF6-gf9sUI7aWX4TQ8RJsAkWAK2379vXMrnf2OcxfaQGBudIa9M0YOgOtPjnh4kC9kA2S_a5Fnz_QhafDbynpTjHgNdI381ktOpwZvvZDfaurWXHXLow5hbLqNL3oD8cb5fk1tx8iU9u7UX9A5l2ycwXBLdZjRu3ATjVHU7pK4Kl955AbCkm5xQcbCtzCmntvJDu9qAPPYjbItzGI9f-0-G0T44UUhmbPD-tlfLO6X1t597f-3Ejq1Dq9Eep9_88KTfnPG5nvql9pPFtRJ_C4L2wxUXJSut4TwULac1KB1fBtpapKKci0kNCZDX6E8rmrGZ-cV-wOFMlxPp91XNGw4Kn4TkjmN98UVpiObzhIqnE2S2DWhuOz1FTuNDFtIaoVvqqS04ThT2niJ_qsUcB9JPK76gBdEsvpYoU44mDfC4S/download [following]\n",
            "--2019-10-07 00:21:49--  https://public.boxcloud.com/d/1/b1!tLdrRqaJe6HDrmjd_xTKdlScFg8GT_u3QEYBQLOeA2zrhpeIODETJAgCteMfo-clukndliLR6HyJVywzpCV-wP1VJ0xT0T__vC6YjrHmI_COMPa5zuwL0ELE7K_SPqIiu7yq1thXecFlAgqyhO45oYblLRczEL1Yzmsp4jPmNctqs6YcibyQPs9R-cofKqE3wt7RGMzF7pxkhMnpnnIYkh4Mr-fRI4f6XuIn38oWAHnTiiwBTb5INc4mWIK1A6pWFCaaCz0hPgR3jxbP4zosR-LnbsFkT-vWsDAqX6hXgk8iOLF-ui-TcWyxzhnssdCOmcGxemnjrGL4mVrtODJWt9-8GoP6-nrEzPq5mkfXCbXZiMWasp84xTTcS0bF6fj6oX1EqL-K7SbJH6PfVEsqCdZmeGhgSJjyP7iZjCButoiDm9waiFPJaDcBtgEsHQbOex-kTr1k6OwoWnxMb9C-AScWmbB1V2cVemXla8t1duE6AQkKg1f2m1d_1bOWZx4PXENDJ_IXizlDp9WqUr8GJ2bVPb8OeHtb6GspWPuv1Kqw4CHrc10xQo3DLI3BjbXNNzl-QdUKE3pBDTl_NABRkPpHsDmGhBlJRJLu5JddSZ1SWCEmCV_rt8xbnhOfUGgCYvJ5jsQYXsXiVdbqFgdgXeGiwy271k3NpBOb2grXmQgs3xud8IKK8N6ugAExPER89BISAeoA5FIDNkwMg-sdb4oRDgNxbFP6CqmqFVWBUTFhyNhFMyYax7TPjpY9ideGpyg4nlWw4x3KYkc1SBsUdv_6g5yuR3rOVv7xgGpAqczfzdj5rSBC3YG-DfHRytJIuc1r6nVSSqFBgZlGrVuLwgjIo_HT2oa5YWP8ZrMt3SJb1dHy0tYhMCc7I-803g1X2WXwab5D5tIM8f0ckqSyAzKhwc_G7ys-WAAf1U_ghE_BQ39-NRTDhoBx7v9lb6bLbZM4Vo3CEx-ZZIXJ5ZHQEr16VXFEfjqwKF6-gf9sUI7aWX4TQ8RJsAkWAK2379vXMrnf2OcxfaQGBudIa9M0YOgOtPjnh4kC9kA2S_a5Fnz_QhafDbynpTjHgNdI381ktOpwZvvZDfaurWXHXLow5hbLqNL3oD8cb5fk1tx8iU9u7UX9A5l2ycwXBLdZjRu3ATjVHU7pK4Kl955AbCkm5xQcbCtzCmntvJDu9qAPPYjbItzGI9f-0-G0T44UUhmbPD-tlfLO6X1t597f-3Ejq1Dq9Eep9_88KTfnPG5nvql9pPFtRJ_C4L2wxUXJSut4TwULac1KB1fBtpapKKci0kNCZDX6E8rmrGZ-cV-wOFMlxPp91XNGw4Kn4TkjmN98UVpiObzhIqnE2S2DWhuOz1FTuNDFtIaoVvqqS04ThT2niJ_qsUcB9JPK76gBdEsvpYoU44mDfC4S/download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 103.116.4.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|103.116.4.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  10.1MB/s    in 1.2s    \n",
            "\n",
            "2019-10-07 00:21:51 (10.1 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqIyTFmDfrM",
        "colab_type": "code",
        "outputId": "723aad90-a581-43e9-9b0b-a9bc76e20da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datasets.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'valid', 'test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NSz3jyD2qsk",
        "colab_type": "code",
        "outputId": "10d8c3ae-438e-413b-dc3d-956fbef80503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('train size:', len(datasets['train']),'\\nvalid size:',len(datasets['valid']),'\\ntest size:',len(datasets['test']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train size: 78274 \n",
            "valid size: 8464 \n",
            "test size: 9708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o_A_F1yDfrT",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r9Q1aFyDfrW",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmHuhyOeDfra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):  #显示进度条Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))#从第二个词开始就是target，shifted version of sequence as  \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "    \n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    pad_token = wiki_dict.get_id('<pad>') # pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceQhyp7tDfrc",
        "colab_type": "code",
        "outputId": "5e4059c7-024e-4a09-d3e5-9de9bb21f754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "wiki_dict = Dictionary(datasets,include_valid=False)\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "encoded = wiki_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wiki_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "tokenized_datasets = tokenize_dataset(datasets, wiki_dict)\n",
        "tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in tokenized_datasets.items():\n",
        "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "tensor_dataset['train'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:11<00:00, 593.10it/s]\n",
            " 18%|█▊        | 14095/78274 [00:00<00:00, 140932.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 94176.86it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 133220.96it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 137731.67it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Q_JFcWDfrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders = {}\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "for split, wiki_dataset in tensor_dataset.items():\n",
        "    loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZfDCOxgDfrZ",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esYzeIQDfrh",
        "colab_type": "text"
      },
      "source": [
        "#### make model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hmRa9u9uBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.RNN (Baseline)\n",
        "class RNN_LM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, lstm and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['dropout'], batch_first=True) #inputs,outputs\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_output,hidden = self.rnn(embeddings) #return a tuple,out的last slice和hidden一样（因为out give you access to all hidden states in the sequence, hidden will allow you to continue the sequence and backpropagate)        \n",
        "        logits = self.projection(rnn_output)#why 0?rnn gives all outputs of all hiddens it has computed so far.\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL6g8qkWDfri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.LSTM\n",
        "class LSTM_LM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, lstm and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['dropout'], batch_first=True) #inputs,outputs\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_output,hidden = self.lstm(embeddings) #out的last slice和hidden一样（因为out give you access to all hidden states in the sequence, hidden will allow you to continue the sequence and backpropagate)\n",
        "        logits = self.projection(lstm_output)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJFaVPJmjlMS",
        "colab_type": "text"
      },
      "source": [
        "##### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KUQspuhDfro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a model, criterion and optimizer\n",
        "#specify which model(RNN or LSTM) or if use a pretrained or not\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "assert current_device == 'cuda'\n",
        "\n",
        "load_pretrained = True\n",
        "model_type = 'rnn'\n",
        "model_path = os.path.join(model_folder_path,'wiki_rnn_lm.pt')\n",
        "\n",
        "if load_pretrained:\n",
        "    if model_type == 'lstm':\n",
        "        if not os.path.exists('wiki_lstm_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_lstm_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "        \n",
        "    if model_type =='rnn':\n",
        "        if not os.path.exists('wiki_rnn_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load(model_path)\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "else:\n",
        "    embedding_dim = 64\n",
        "    hidden_size = 128\n",
        "    num_layers = 2\n",
        "    dropout = 0.1\n",
        "\n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout': dropout,\n",
        "    }\n",
        "    if model_type == 'lstm':\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "    if model_type == 'rnn':\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'),reduction='sum')\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeB49BNTDfrq",
        "colab_type": "code",
        "outputId": "bfc80961-32db-4dba-9e9d-072791684e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_LM(\n",
              "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
              "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7de25b97-9e70-4716-e7b0-ed1e61ee95d0",
        "id": "O0_s1CYkGgu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(10):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        \n",
        "        train_loss_cache = 0\n",
        "        train_non_pad_tokens_cache = 0\n",
        "        \n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            train_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            train_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "            loss /= non_pad_tokens  # very important to normalize your current loss before you run .backward()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                        \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "            \n",
        "    #do valid\n",
        "\n",
        "    valid_loss_cache = 0\n",
        "    valid_non_pad_tokens_cache = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            valid_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "        avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
        "            \n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "if model_type == 'lstm':\n",
        "    lstm_plot_cache = plot_cache\n",
        "if model_type == 'rnn':\n",
        "    rnn_plot_cache = plot_cache"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4288\n",
            "Step 100 avg train loss = 7.7435\n",
            "Step 200 avg train loss = 7.3367\n",
            "Step 300 avg train loss = 7.1327\n",
            "Step 400 avg train loss = 7.0006\n",
            "Step 500 avg train loss = 6.8983\n",
            "Step 600 avg train loss = 6.8192\n",
            "Step 700 avg train loss = 6.7519\n",
            "Step 800 avg train loss = 6.6939\n",
            "Step 900 avg train loss = 6.6447\n",
            "Step 1000 avg train loss = 6.6010\n",
            "Step 1100 avg train loss = 6.5596\n",
            "Step 1200 avg train loss = 6.5238\n",
            "Step 1300 avg train loss = 6.4894\n",
            "Step 1400 avg train loss = 6.4585\n",
            "Step 1500 avg train loss = 6.4313\n",
            "Step 1600 avg train loss = 6.4043\n",
            "Step 1700 avg train loss = 6.3803\n",
            "Step 1800 avg train loss = 6.3568\n",
            "Step 1900 avg train loss = 6.3340\n",
            "Step 2000 avg train loss = 6.3138\n",
            "Step 2100 avg train loss = 6.2937\n",
            "Step 2200 avg train loss = 6.2757\n",
            "Step 2300 avg train loss = 6.2585\n",
            "Step 2400 avg train loss = 6.2411\n",
            "Validation loss after 0 epoch = 5.6907\n",
            "Step 0 avg train loss = 5.5617\n",
            "Step 100 avg train loss = 5.7252\n",
            "Step 200 avg train loss = 5.7123\n",
            "Step 300 avg train loss = 5.7101\n",
            "Step 400 avg train loss = 5.7114\n",
            "Step 500 avg train loss = 5.7071\n",
            "Step 600 avg train loss = 5.7050\n",
            "Step 700 avg train loss = 5.6985\n",
            "Step 800 avg train loss = 5.6952\n",
            "Step 900 avg train loss = 5.6936\n",
            "Step 1000 avg train loss = 5.6903\n",
            "Step 1100 avg train loss = 5.6854\n",
            "Step 1200 avg train loss = 5.6822\n",
            "Step 1300 avg train loss = 5.6798\n",
            "Step 1400 avg train loss = 5.6766\n",
            "Step 1500 avg train loss = 5.6743\n",
            "Step 1600 avg train loss = 5.6716\n",
            "Step 1700 avg train loss = 5.6666\n",
            "Step 1800 avg train loss = 5.6608\n",
            "Step 1900 avg train loss = 5.6568\n",
            "Step 2000 avg train loss = 5.6536\n",
            "Step 2100 avg train loss = 5.6490\n",
            "Step 2200 avg train loss = 5.6460\n",
            "Step 2300 avg train loss = 5.6430\n",
            "Step 2400 avg train loss = 5.6389\n",
            "Validation loss after 1 epoch = 5.5088\n",
            "Step 0 avg train loss = 5.2260\n",
            "Step 100 avg train loss = 5.3873\n",
            "Step 200 avg train loss = 5.3914\n",
            "Step 300 avg train loss = 5.3962\n",
            "Step 400 avg train loss = 5.3966\n",
            "Step 500 avg train loss = 5.4003\n",
            "Step 600 avg train loss = 5.4021\n",
            "Step 700 avg train loss = 5.4051\n",
            "Step 800 avg train loss = 5.4045\n",
            "Step 900 avg train loss = 5.4045\n",
            "Step 1000 avg train loss = 5.4032\n",
            "Step 1100 avg train loss = 5.4032\n",
            "Step 1200 avg train loss = 5.4018\n",
            "Step 1300 avg train loss = 5.4021\n",
            "Step 1400 avg train loss = 5.4038\n",
            "Step 1500 avg train loss = 5.4022\n",
            "Step 1600 avg train loss = 5.4029\n",
            "Step 1700 avg train loss = 5.4016\n",
            "Step 1800 avg train loss = 5.4017\n",
            "Step 1900 avg train loss = 5.4002\n",
            "Step 2000 avg train loss = 5.3986\n",
            "Step 2100 avg train loss = 5.3973\n",
            "Step 2200 avg train loss = 5.3964\n",
            "Step 2300 avg train loss = 5.3957\n",
            "Step 2400 avg train loss = 5.3955\n",
            "Validation loss after 2 epoch = 5.4235\n",
            "Step 0 avg train loss = 5.3070\n",
            "Step 100 avg train loss = 5.2171\n",
            "Step 200 avg train loss = 5.2107\n",
            "Step 300 avg train loss = 5.2153\n",
            "Step 400 avg train loss = 5.2164\n",
            "Step 500 avg train loss = 5.2230\n",
            "Step 600 avg train loss = 5.2229\n",
            "Step 700 avg train loss = 5.2198\n",
            "Step 800 avg train loss = 5.2236\n",
            "Step 900 avg train loss = 5.2222\n",
            "Step 1000 avg train loss = 5.2233\n",
            "Step 1100 avg train loss = 5.2246\n",
            "Step 1200 avg train loss = 5.2272\n",
            "Step 1300 avg train loss = 5.2286\n",
            "Step 1400 avg train loss = 5.2291\n",
            "Step 1500 avg train loss = 5.2299\n",
            "Step 1600 avg train loss = 5.2301\n",
            "Step 1700 avg train loss = 5.2307\n",
            "Step 1800 avg train loss = 5.2301\n",
            "Step 1900 avg train loss = 5.2310\n",
            "Step 2000 avg train loss = 5.2319\n",
            "Step 2100 avg train loss = 5.2329\n",
            "Step 2200 avg train loss = 5.2323\n",
            "Step 2300 avg train loss = 5.2326\n",
            "Step 2400 avg train loss = 5.2317\n",
            "Validation loss after 3 epoch = 5.3817\n",
            "Step 0 avg train loss = 5.1949\n",
            "Step 100 avg train loss = 5.0591\n",
            "Step 200 avg train loss = 5.0743\n",
            "Step 300 avg train loss = 5.0767\n",
            "Step 400 avg train loss = 5.0814\n",
            "Step 500 avg train loss = 5.0910\n",
            "Step 600 avg train loss = 5.0940\n",
            "Step 700 avg train loss = 5.0953\n",
            "Step 800 avg train loss = 5.0944\n",
            "Step 900 avg train loss = 5.0976\n",
            "Step 1000 avg train loss = 5.0988\n",
            "Step 1100 avg train loss = 5.1006\n",
            "Step 1200 avg train loss = 5.1016\n",
            "Step 1300 avg train loss = 5.1031\n",
            "Step 1400 avg train loss = 5.1051\n",
            "Step 1500 avg train loss = 5.1057\n",
            "Step 1600 avg train loss = 5.1070\n",
            "Step 1700 avg train loss = 5.1073\n",
            "Step 1800 avg train loss = 5.1089\n",
            "Step 1900 avg train loss = 5.1091\n",
            "Step 2000 avg train loss = 5.1096\n",
            "Step 2100 avg train loss = 5.1095\n",
            "Step 2200 avg train loss = 5.1116\n",
            "Step 2300 avg train loss = 5.1119\n",
            "Step 2400 avg train loss = 5.1124\n",
            "Validation loss after 4 epoch = 5.3536\n",
            "Step 0 avg train loss = 5.1118\n",
            "Step 100 avg train loss = 4.9606\n",
            "Step 200 avg train loss = 4.9744\n",
            "Step 300 avg train loss = 4.9781\n",
            "Step 400 avg train loss = 4.9826\n",
            "Step 500 avg train loss = 4.9829\n",
            "Step 600 avg train loss = 4.9834\n",
            "Step 700 avg train loss = 4.9875\n",
            "Step 800 avg train loss = 4.9902\n",
            "Step 900 avg train loss = 4.9921\n",
            "Step 1000 avg train loss = 4.9919\n",
            "Step 1100 avg train loss = 4.9930\n",
            "Step 1200 avg train loss = 4.9963\n",
            "Step 1300 avg train loss = 4.9989\n",
            "Step 1400 avg train loss = 5.0014\n",
            "Step 1500 avg train loss = 5.0027\n",
            "Step 1600 avg train loss = 5.0038\n",
            "Step 1700 avg train loss = 5.0055\n",
            "Step 1800 avg train loss = 5.0080\n",
            "Step 1900 avg train loss = 5.0096\n",
            "Step 2000 avg train loss = 5.0096\n",
            "Step 2100 avg train loss = 5.0115\n",
            "Step 2200 avg train loss = 5.0139\n",
            "Step 2300 avg train loss = 5.0151\n",
            "Step 2400 avg train loss = 5.0164\n",
            "Validation loss after 5 epoch = 5.3390\n",
            "Step 0 avg train loss = 4.6192\n",
            "Step 100 avg train loss = 4.8753\n",
            "Step 200 avg train loss = 4.8806\n",
            "Step 300 avg train loss = 4.8826\n",
            "Step 400 avg train loss = 4.8908\n",
            "Step 500 avg train loss = 4.8952\n",
            "Step 600 avg train loss = 4.8975\n",
            "Step 700 avg train loss = 4.9010\n",
            "Step 800 avg train loss = 4.9045\n",
            "Step 900 avg train loss = 4.9082\n",
            "Step 1000 avg train loss = 4.9084\n",
            "Step 1100 avg train loss = 4.9104\n",
            "Step 1200 avg train loss = 4.9135\n",
            "Step 1300 avg train loss = 4.9148\n",
            "Step 1400 avg train loss = 4.9185\n",
            "Step 1500 avg train loss = 4.9204\n",
            "Step 1600 avg train loss = 4.9213\n",
            "Step 1700 avg train loss = 4.9223\n",
            "Step 1800 avg train loss = 4.9232\n",
            "Step 1900 avg train loss = 4.9249\n",
            "Step 2000 avg train loss = 4.9271\n",
            "Step 2100 avg train loss = 4.9283\n",
            "Step 2200 avg train loss = 4.9304\n",
            "Step 2300 avg train loss = 4.9330\n",
            "Step 2400 avg train loss = 4.9356\n",
            "Validation loss after 6 epoch = 5.3320\n",
            "Step 0 avg train loss = 4.8354\n",
            "Step 100 avg train loss = 4.8200\n",
            "Step 200 avg train loss = 4.8152\n",
            "Step 300 avg train loss = 4.8197\n",
            "Step 400 avg train loss = 4.8274\n",
            "Step 500 avg train loss = 4.8300\n",
            "Step 600 avg train loss = 4.8343\n",
            "Step 700 avg train loss = 4.8388\n",
            "Step 800 avg train loss = 4.8408\n",
            "Step 900 avg train loss = 4.8410\n",
            "Step 1000 avg train loss = 4.8426\n",
            "Step 1100 avg train loss = 4.8463\n",
            "Step 1200 avg train loss = 4.8494\n",
            "Step 1300 avg train loss = 4.8520\n",
            "Step 1400 avg train loss = 4.8527\n",
            "Step 1500 avg train loss = 4.8524\n",
            "Step 1600 avg train loss = 4.8544\n",
            "Step 1700 avg train loss = 4.8562\n",
            "Step 1800 avg train loss = 4.8586\n",
            "Step 1900 avg train loss = 4.8610\n",
            "Step 2000 avg train loss = 4.8638\n",
            "Step 2100 avg train loss = 4.8646\n",
            "Step 2200 avg train loss = 4.8665\n",
            "Step 2300 avg train loss = 4.8682\n",
            "Step 2400 avg train loss = 4.8697\n",
            "Validation loss after 7 epoch = 5.3329\n",
            "Step 0 avg train loss = 4.6443\n",
            "Step 100 avg train loss = 4.7217\n",
            "Step 200 avg train loss = 4.7359\n",
            "Step 300 avg train loss = 4.7474\n",
            "Step 400 avg train loss = 4.7591\n",
            "Step 500 avg train loss = 4.7710\n",
            "Step 600 avg train loss = 4.7733\n",
            "Step 700 avg train loss = 4.7784\n",
            "Step 800 avg train loss = 4.7813\n",
            "Step 900 avg train loss = 4.7821\n",
            "Step 1000 avg train loss = 4.7849\n",
            "Step 1100 avg train loss = 4.7885\n",
            "Step 1200 avg train loss = 4.7902\n",
            "Step 1300 avg train loss = 4.7916\n",
            "Step 1400 avg train loss = 4.7948\n",
            "Step 1500 avg train loss = 4.7992\n",
            "Step 1600 avg train loss = 4.8019\n",
            "Step 1700 avg train loss = 4.8037\n",
            "Step 1800 avg train loss = 4.8067\n",
            "Step 1900 avg train loss = 4.8092\n",
            "Step 2000 avg train loss = 4.8105\n",
            "Step 2100 avg train loss = 4.8122\n",
            "Step 2200 avg train loss = 4.8144\n",
            "Step 2300 avg train loss = 4.8157\n",
            "Step 2400 avg train loss = 4.8172\n",
            "Validation loss after 8 epoch = 5.3305\n",
            "Step 0 avg train loss = 4.6659\n",
            "Step 100 avg train loss = 4.6676\n",
            "Step 200 avg train loss = 4.6970\n",
            "Step 300 avg train loss = 4.6956\n",
            "Step 400 avg train loss = 4.7083\n",
            "Step 500 avg train loss = 4.7159\n",
            "Step 600 avg train loss = 4.7188\n",
            "Step 700 avg train loss = 4.7210\n",
            "Step 800 avg train loss = 4.7234\n",
            "Step 900 avg train loss = 4.7267\n",
            "Step 1000 avg train loss = 4.7286\n",
            "Step 1100 avg train loss = 4.7333\n",
            "Step 1200 avg train loss = 4.7367\n",
            "Step 1300 avg train loss = 4.7387\n",
            "Step 1400 avg train loss = 4.7405\n",
            "Step 1500 avg train loss = 4.7435\n",
            "Step 1600 avg train loss = 4.7455\n",
            "Step 1700 avg train loss = 4.7470\n",
            "Step 1800 avg train loss = 4.7490\n",
            "Step 1900 avg train loss = 4.7509\n",
            "Step 2000 avg train loss = 4.7526\n",
            "Step 2100 avg train loss = 4.7555\n",
            "Step 2200 avg train loss = 4.7581\n",
            "Step 2300 avg train loss = 4.7608\n",
            "Step 2400 avg train loss = 4.7630\n",
            "Validation loss after 9 epoch = 5.3316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBj_zAnoeQJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model, like pickle dump\n",
        "if not load_pretrained:\n",
        "    torch.save({\n",
        "        'options': options,\n",
        "        'loss_cache': plot_cache,\n",
        "        'model_dict': model.state_dict() #all params\n",
        "    }, model_path) #file name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-adwOIaTJ4",
        "colab_type": "text"
      },
      "source": [
        " ##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYnnYJnmFnSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_pretrained = False\n",
        "model_type = 'lstm'\n",
        "# creating a model, criterion and optimizer\n",
        "#specify which model(RNN or LSTM) or if use a pretrained or not\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "assert current_device == 'cuda'\n",
        "\n",
        "if load_pretrained:\n",
        "    if model_type == 'lstm':\n",
        "        if not os.path.exists('wiki_lstm_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_lstm_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "        \n",
        "    if model_type =='rnn':\n",
        "        if not os.path.exists('wiki_rnn_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_rnn_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "else:\n",
        "    embedding_dim = 64\n",
        "    hidden_size = 128\n",
        "    num_layers = 2\n",
        "    dropout = 0.1\n",
        "\n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout': dropout,\n",
        "    }\n",
        "    if model_type == 'lstm':\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "    if model_type == 'rnn':\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'),reduction='sum')\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmpVbKLVZ_vv",
        "colab_type": "code",
        "outputId": "67973602-0596-4db6-9179-717797a7c467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_LM(\n",
              "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9bDa_caPdq",
        "colab_type": "code",
        "outputId": "bd6a15e7-8da4-467f-cbaf-5055df1c3c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(10):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        \n",
        "        train_loss_cache = 0\n",
        "        train_non_pad_tokens_cache = 0\n",
        "        \n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            train_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            train_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "            loss /= non_pad_tokens  # very important to normalize your current loss before you run .backward()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                        \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "            \n",
        "    #do valid\n",
        "\n",
        "    valid_loss_cache = 0\n",
        "    valid_non_pad_tokens_cache = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            valid_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "        avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
        "            \n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "if model_type == 'lstm':\n",
        "    lstm_plot_cache = plot_cache\n",
        "if model_type == 'rnn':\n",
        "    rnn_plot_cache = plot_cache"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4010\n",
            "Step 100 avg train loss = 7.8683\n",
            "Step 200 avg train loss = 7.5247\n",
            "Step 300 avg train loss = 7.3656\n",
            "Step 400 avg train loss = 7.2560\n",
            "Step 500 avg train loss = 7.1702\n",
            "Step 600 avg train loss = 7.0986\n",
            "Step 700 avg train loss = 7.0338\n",
            "Step 800 avg train loss = 6.9773\n",
            "Step 900 avg train loss = 6.9283\n",
            "Step 1000 avg train loss = 6.8843\n",
            "Step 1100 avg train loss = 6.8434\n",
            "Step 1200 avg train loss = 6.8054\n",
            "Step 1300 avg train loss = 6.7713\n",
            "Step 1400 avg train loss = 6.7398\n",
            "Step 1500 avg train loss = 6.7079\n",
            "Step 1600 avg train loss = 6.6802\n",
            "Step 1700 avg train loss = 6.6537\n",
            "Step 1800 avg train loss = 6.6285\n",
            "Step 1900 avg train loss = 6.6045\n",
            "Step 2000 avg train loss = 6.5809\n",
            "Step 2100 avg train loss = 6.5583\n",
            "Step 2200 avg train loss = 6.5378\n",
            "Step 2300 avg train loss = 6.5175\n",
            "Step 2400 avg train loss = 6.4974\n",
            "Validation loss after 0 epoch = 5.8609\n",
            "Step 0 avg train loss = 6.1006\n",
            "Step 100 avg train loss = 5.9409\n",
            "Step 200 avg train loss = 5.9408\n",
            "Step 300 avg train loss = 5.9358\n",
            "Step 400 avg train loss = 5.9277\n",
            "Step 500 avg train loss = 5.9242\n",
            "Step 600 avg train loss = 5.9179\n",
            "Step 700 avg train loss = 5.9092\n",
            "Step 800 avg train loss = 5.9044\n",
            "Step 900 avg train loss = 5.8967\n",
            "Step 1000 avg train loss = 5.8908\n",
            "Step 1100 avg train loss = 5.8863\n",
            "Step 1200 avg train loss = 5.8805\n",
            "Step 1300 avg train loss = 5.8730\n",
            "Step 1400 avg train loss = 5.8659\n",
            "Step 1500 avg train loss = 5.8601\n",
            "Step 1600 avg train loss = 5.8545\n",
            "Step 1700 avg train loss = 5.8476\n",
            "Step 1800 avg train loss = 5.8414\n",
            "Step 1900 avg train loss = 5.8365\n",
            "Step 2000 avg train loss = 5.8303\n",
            "Step 2100 avg train loss = 5.8247\n",
            "Step 2200 avg train loss = 5.8186\n",
            "Step 2300 avg train loss = 5.8135\n",
            "Step 2400 avg train loss = 5.8080\n",
            "Validation loss after 1 epoch = 5.5736\n",
            "Step 0 avg train loss = 5.8299\n",
            "Step 100 avg train loss = 5.5912\n",
            "Step 200 avg train loss = 5.5815\n",
            "Step 300 avg train loss = 5.5837\n",
            "Step 400 avg train loss = 5.5838\n",
            "Step 500 avg train loss = 5.5819\n",
            "Step 600 avg train loss = 5.5758\n",
            "Step 700 avg train loss = 5.5747\n",
            "Step 800 avg train loss = 5.5705\n",
            "Step 900 avg train loss = 5.5680\n",
            "Step 1000 avg train loss = 5.5649\n",
            "Step 1100 avg train loss = 5.5611\n",
            "Step 1200 avg train loss = 5.5583\n",
            "Step 1300 avg train loss = 5.5553\n",
            "Step 1400 avg train loss = 5.5524\n",
            "Step 1500 avg train loss = 5.5489\n",
            "Step 1600 avg train loss = 5.5477\n",
            "Step 1700 avg train loss = 5.5464\n",
            "Step 1800 avg train loss = 5.5436\n",
            "Step 1900 avg train loss = 5.5412\n",
            "Step 2000 avg train loss = 5.5384\n",
            "Step 2100 avg train loss = 5.5356\n",
            "Step 2200 avg train loss = 5.5326\n",
            "Step 2300 avg train loss = 5.5299\n",
            "Step 2400 avg train loss = 5.5269\n",
            "Validation loss after 2 epoch = 5.4462\n",
            "Step 0 avg train loss = 5.3037\n",
            "Step 100 avg train loss = 5.3486\n",
            "Step 200 avg train loss = 5.3529\n",
            "Step 300 avg train loss = 5.3492\n",
            "Step 400 avg train loss = 5.3510\n",
            "Step 500 avg train loss = 5.3537\n",
            "Step 600 avg train loss = 5.3511\n",
            "Step 700 avg train loss = 5.3472\n",
            "Step 800 avg train loss = 5.3497\n",
            "Step 900 avg train loss = 5.3495\n",
            "Step 1000 avg train loss = 5.3473\n",
            "Step 1100 avg train loss = 5.3463\n",
            "Step 1200 avg train loss = 5.3443\n",
            "Step 1300 avg train loss = 5.3433\n",
            "Step 1400 avg train loss = 5.3426\n",
            "Step 1500 avg train loss = 5.3433\n",
            "Step 1600 avg train loss = 5.3439\n",
            "Step 1700 avg train loss = 5.3427\n",
            "Step 1800 avg train loss = 5.3415\n",
            "Step 1900 avg train loss = 5.3408\n",
            "Step 2000 avg train loss = 5.3384\n",
            "Step 2100 avg train loss = 5.3376\n",
            "Step 2200 avg train loss = 5.3368\n",
            "Step 2300 avg train loss = 5.3360\n",
            "Step 2400 avg train loss = 5.3345\n",
            "Validation loss after 3 epoch = 5.3765\n",
            "Step 0 avg train loss = 5.0519\n",
            "Step 100 avg train loss = 5.1814\n",
            "Step 200 avg train loss = 5.1863\n",
            "Step 300 avg train loss = 5.1962\n",
            "Step 400 avg train loss = 5.1930\n",
            "Step 500 avg train loss = 5.1968\n",
            "Step 600 avg train loss = 5.1992\n",
            "Step 700 avg train loss = 5.1979\n",
            "Step 800 avg train loss = 5.1992\n",
            "Step 900 avg train loss = 5.1976\n",
            "Step 1000 avg train loss = 5.1983\n",
            "Step 1100 avg train loss = 5.1967\n",
            "Step 1200 avg train loss = 5.1949\n",
            "Step 1300 avg train loss = 5.1952\n",
            "Step 1400 avg train loss = 5.1946\n",
            "Step 1500 avg train loss = 5.1934\n",
            "Step 1600 avg train loss = 5.1923\n",
            "Step 1700 avg train loss = 5.1908\n",
            "Step 1800 avg train loss = 5.1898\n",
            "Step 1900 avg train loss = 5.1891\n",
            "Step 2000 avg train loss = 5.1871\n",
            "Step 2100 avg train loss = 5.1862\n",
            "Step 2200 avg train loss = 5.1851\n",
            "Step 2300 avg train loss = 5.1846\n",
            "Step 2400 avg train loss = 5.1845\n",
            "Validation loss after 4 epoch = 5.3353\n",
            "Step 0 avg train loss = 5.1574\n",
            "Step 100 avg train loss = 5.0725\n",
            "Step 200 avg train loss = 5.0590\n",
            "Step 300 avg train loss = 5.0557\n",
            "Step 400 avg train loss = 5.0560\n",
            "Step 500 avg train loss = 5.0513\n",
            "Step 600 avg train loss = 5.0532\n",
            "Step 700 avg train loss = 5.0554\n",
            "Step 800 avg train loss = 5.0548\n",
            "Step 900 avg train loss = 5.0548\n",
            "Step 1000 avg train loss = 5.0572\n",
            "Step 1100 avg train loss = 5.0581\n",
            "Step 1200 avg train loss = 5.0584\n",
            "Step 1300 avg train loss = 5.0590\n",
            "Step 1400 avg train loss = 5.0583\n",
            "Step 1500 avg train loss = 5.0573\n",
            "Step 1600 avg train loss = 5.0570\n",
            "Step 1700 avg train loss = 5.0574\n",
            "Step 1800 avg train loss = 5.0578\n",
            "Step 1900 avg train loss = 5.0583\n",
            "Step 2000 avg train loss = 5.0588\n",
            "Step 2100 avg train loss = 5.0590\n",
            "Step 2200 avg train loss = 5.0589\n",
            "Step 2300 avg train loss = 5.0596\n",
            "Step 2400 avg train loss = 5.0597\n",
            "Validation loss after 5 epoch = 5.3130\n",
            "Step 0 avg train loss = 4.9738\n",
            "Step 100 avg train loss = 4.9154\n",
            "Step 200 avg train loss = 4.9254\n",
            "Step 300 avg train loss = 4.9296\n",
            "Step 400 avg train loss = 4.9349\n",
            "Step 500 avg train loss = 4.9372\n",
            "Step 600 avg train loss = 4.9399\n",
            "Step 700 avg train loss = 4.9401\n",
            "Step 800 avg train loss = 4.9426\n",
            "Step 900 avg train loss = 4.9447\n",
            "Step 1000 avg train loss = 4.9450\n",
            "Step 1100 avg train loss = 4.9460\n",
            "Step 1200 avg train loss = 4.9474\n",
            "Step 1300 avg train loss = 4.9480\n",
            "Step 1400 avg train loss = 4.9497\n",
            "Step 1500 avg train loss = 4.9508\n",
            "Step 1600 avg train loss = 4.9493\n",
            "Step 1700 avg train loss = 4.9503\n",
            "Step 1800 avg train loss = 4.9508\n",
            "Step 1900 avg train loss = 4.9527\n",
            "Step 2000 avg train loss = 4.9535\n",
            "Step 2100 avg train loss = 4.9541\n",
            "Step 2200 avg train loss = 4.9536\n",
            "Step 2300 avg train loss = 4.9541\n",
            "Step 2400 avg train loss = 4.9551\n",
            "Validation loss after 6 epoch = 5.3046\n",
            "Step 0 avg train loss = 4.7610\n",
            "Step 100 avg train loss = 4.8376\n",
            "Step 200 avg train loss = 4.8455\n",
            "Step 300 avg train loss = 4.8471\n",
            "Step 400 avg train loss = 4.8482\n",
            "Step 500 avg train loss = 4.8538\n",
            "Step 600 avg train loss = 4.8552\n",
            "Step 700 avg train loss = 4.8569\n",
            "Step 800 avg train loss = 4.8576\n",
            "Step 900 avg train loss = 4.8557\n",
            "Step 1000 avg train loss = 4.8568\n",
            "Step 1100 avg train loss = 4.8577\n",
            "Step 1200 avg train loss = 4.8573\n",
            "Step 1300 avg train loss = 4.8586\n",
            "Step 1400 avg train loss = 4.8602\n",
            "Step 1500 avg train loss = 4.8605\n",
            "Step 1600 avg train loss = 4.8614\n",
            "Step 1700 avg train loss = 4.8619\n",
            "Step 1800 avg train loss = 4.8622\n",
            "Step 1900 avg train loss = 4.8629\n",
            "Step 2000 avg train loss = 4.8645\n",
            "Step 2100 avg train loss = 4.8656\n",
            "Step 2200 avg train loss = 4.8668\n",
            "Step 2300 avg train loss = 4.8677\n",
            "Step 2400 avg train loss = 4.8677\n",
            "Validation loss after 7 epoch = 5.3077\n",
            "Step 0 avg train loss = 4.5050\n",
            "Step 100 avg train loss = 4.7665\n",
            "Step 200 avg train loss = 4.7629\n",
            "Step 300 avg train loss = 4.7637\n",
            "Step 400 avg train loss = 4.7699\n",
            "Step 500 avg train loss = 4.7703\n",
            "Step 600 avg train loss = 4.7730\n",
            "Step 700 avg train loss = 4.7724\n",
            "Step 800 avg train loss = 4.7743\n",
            "Step 900 avg train loss = 4.7777\n",
            "Step 1000 avg train loss = 4.7767\n",
            "Step 1100 avg train loss = 4.7798\n",
            "Step 1200 avg train loss = 4.7797\n",
            "Step 1300 avg train loss = 4.7823\n",
            "Step 1400 avg train loss = 4.7833\n",
            "Step 1500 avg train loss = 4.7862\n",
            "Step 1600 avg train loss = 4.7870\n",
            "Step 1700 avg train loss = 4.7881\n",
            "Step 1800 avg train loss = 4.7883\n",
            "Step 1900 avg train loss = 4.7889\n",
            "Step 2000 avg train loss = 4.7898\n",
            "Step 2100 avg train loss = 4.7913\n",
            "Step 2200 avg train loss = 4.7914\n",
            "Step 2300 avg train loss = 4.7925\n",
            "Step 2400 avg train loss = 4.7925\n",
            "Validation loss after 8 epoch = 5.3109\n",
            "Step 0 avg train loss = 4.5413\n",
            "Step 100 avg train loss = 4.6860\n",
            "Step 200 avg train loss = 4.6852\n",
            "Step 300 avg train loss = 4.6917\n",
            "Step 400 avg train loss = 4.6947\n",
            "Step 500 avg train loss = 4.7011\n",
            "Step 600 avg train loss = 4.7046\n",
            "Step 700 avg train loss = 4.7073\n",
            "Step 800 avg train loss = 4.7057\n",
            "Step 900 avg train loss = 4.7076\n",
            "Step 1000 avg train loss = 4.7086\n",
            "Step 1100 avg train loss = 4.7085\n",
            "Step 1200 avg train loss = 4.7102\n",
            "Step 1300 avg train loss = 4.7123\n",
            "Step 1400 avg train loss = 4.7137\n",
            "Step 1500 avg train loss = 4.7142\n",
            "Step 1600 avg train loss = 4.7172\n",
            "Step 1700 avg train loss = 4.7185\n",
            "Step 1800 avg train loss = 4.7202\n",
            "Step 1900 avg train loss = 4.7214\n",
            "Step 2000 avg train loss = 4.7226\n",
            "Step 2100 avg train loss = 4.7246\n",
            "Step 2200 avg train loss = 4.7244\n",
            "Step 2300 avg train loss = 4.7258\n",
            "Step 2400 avg train loss = 4.7263\n",
            "Validation loss after 9 epoch = 5.3189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpC7irn10bEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model, like pickle dump\n",
        "model_path = os.path.join(model_folder_path,'hw2_2.1.1_lstm_lm.pt')\n",
        "if not load_pretrained:\n",
        "    torch.save({\n",
        "        'options': options,\n",
        "        'loss_cache': plot_cache,\n",
        "        'model_dict': model.state_dict() #all params\n",
        "    }, model_path) #file name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb2JEbttDfrt",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piSCUkz6Dfru",
        "colab_type": "code",
        "outputId": "b04c8ecb-6205-49dd-e435-1c52e1d80429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [i[0] for i in lstm_plot_cache], label='LSTM Train loss')\n",
        "plt.plot(epochs, [i[1] for i in lstm_plot_cache], label='LSTM Valid loss')\n",
        "plt.plot(epochs, [i[0] for i in rnn_plot_cache], label='RNN Train loss')\n",
        "plt.plot(epochs, [i[1] for i in rnn_plot_cache], label='RNN Valid loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Loss curves of RNN/LSTM')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXt5DYN0OuM",
        "colab_type": "code",
        "outputId": "8528141d-20af-4db2-dea2-e1f5d9d286d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in lstm_plot_cache], label='LSTM Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in lstm_plot_cache], label='LSTM Valid ppl')\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in rnn_plot_cache], label='RNN Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in rnn_plot_cache], label='RNN Valid ppl')\n",
        "         \n",
        "plt.legend()\n",
        "plt.title('PPL curves of RNN/LSTM')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n55zrcf8D5f2tMONfsdoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyoTgq1L5XNB",
        "colab_type": "code",
        "outputId": "bad7187a-19d1-4ca8-8fed-e0201ef3f264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('LSTM PPL:',2**(lstm_plot_cache[-1][1]/numpy.log(2)),'RNN PPL',2**(rnn_plot_cache[-1][1]/numpy.log(2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM PPL: 204.16428943493878 RNN PPL 206.77883783443752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCryReIvDfrx",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zjxZThSDfry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsVFpSyDfr0",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BG-64bZDfr1",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRNFbTpbDfr1",
        "colab_type": "code",
        "outputId": "fffa715e-c30b-42b6-e99a-1e48f283ba74",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5pA6188Dfr4",
        "colab_type": "code",
        "outputId": "c0b05ed6-4daa-4a30-8d41-0c9da2627616",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABI0AAARiCAYAAAAp2gdjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3W+s3md93/HPVTs2HhZxssDBCRsekBnwqBz7LIqGthw3tdxpaMlQtnZCwhFFroIA8WDWjCIt0vZgXs00NrFoymCq205ytIiFbAKlwdsBDaVTbUyTOOCZBNPhWIHSONFJHTUO1x7knmX7e+zj5L5/PvjweknR/e8657qO/H301u93p/XeAwAAAABn+6XFPgAAAAAAP39EIwAAAAAK0QgAAACAQjQCAAAAoBCNAAAAAChEIwAAAAAK0QgAAACAQjQCAAAAoBCNAAAAAChEIwAAAACK5Yt9gIu57rrr+rp16xb7GJznpZdeypvf/ObFPgZLkNliSOaLoZgthmK2GIrZYihm68px8ODBP+29v3WhdT/X0WjdunU5cODAYh+D88zOzmZmZmaxj8ESZLYYkvliKGaLoZgthmK2GIrZunK01n54KevcngYAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAwFh67/nZz3622MdgwkQjAAAA4HU7duxY3ve+9+UTn/hENm3alD179mR6ejobNmzIvffee2bdunXrcu+992bTpk35wAc+kO9973uLeGpeD9EIAAAAeEOOHDmSj370ozl06FDuvvvuHDhwII8//ni+8Y1v5PHHHz+z7rrrrsu3v/3t3H333fnc5z63iCfm9RCNAAAAgDfkne98Z2655ZYkyezsbDZt2pSbbrophw8fzlNPPXVm3Yc//OEkyebNm3Ps2LHFOCpvwPLFPgAAAABwZXjo0PHseeRInj15Ktf2F/LqspVJkh/84Ad54IEH8uSTT+aaa67JXXfdlZdffvnMz61c+dq6ZcuW5fTp04tydl4/VxoBAAAAC3ro0PF89stP5PjJU+lJnnvx5Tz34st56NDxvPjii3nTm96Uq6++Os8991y+9rWvLfZxmQBXGgEAAAAL2vPIkZx65dVz3uu9Z88jR/KtXb+SG2+8MRs2bMi73vWufPCDH1ykUzJJohEAAACwoGdPnjrn9fKrp3L9b9535v1du3ZlZmam/NzZ32E0PT2d2dnZAU/JJLk9DQAAAFjQ9WtWva73ufKJRgAAAMCCdm5bn1VXLTvnvVVXLcvObesX6UQMze1pAAAAwILuuOmGJDnzf0+7fs2q7Ny2/sz7LD2iEQAAAHBJ7rjpBpHoF4jb0wAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoRCMAAAAACtEIAAAAgEI0AgAAAKAQjQAAAAAoJhKNWmu/1lo70lr7fmtt1zyfr2ytPTD6/H+31tZNYl8AAAAAhjF2NGqtLUvy75P83STvT/KPW2vvP2/ZbyZ5vvf+niT/Jsm/GndfAAAAAIYziSuNbk7y/d77M733v0iyL8nt5625Pcne0fMHk9zWWmsT2BsAAACAAUwiGt2Q5P+e9fpHo/fmXdN7P53khSR/eQJ7AwAAADCA5RP4HfNdMdTfwJrXFra2I8mOJJmamsrs7OxYh2Py5ubm/LswCLPFkMwXQzFbDMVsMRSzxVDM1tIziWj0oyR/5azX70jy7AXW/Ki1tjzJ1Un+bL5f1nu/P8n9STI9Pd1nZmYmcEQmaXZ2Nv5dGILZYkjmi6GYLYZithiK2WIoZmvpmcTtaX+U5MbW2l9rra1I8htJHj5vzcNJto+e35nkf/Te573SCAAAAIDFN/aVRr330621TyZ5JMmyJP+p9364tfbPkxzovT+c5EtJfq+19v28doXRb4y7LwAAAADDmcTtaem9fzXJV89775+d9fzlJP9wEnsBAAAAMLxJ3J4GAAAAwBIjGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABcwMmTJ3PfffclSWZnZ/OhD31okU90+YhGAAAAABdwdjT6RSMaAQAAAFzArl278vTTT2fjxo3ZuXNn5ubmcuedd+a9731vPvKRj6T3niQ5ePBgbr311mzevDnbtm3LiRMnFvnk4xONAAAAAC5g9+7defe7353vfOc72bNnTw4dOpTPf/7zeeqpp/LMM8/kW9/6Vl555ZV86lOfyoMPPpiDBw/mYx/7WO65557FPvrYli/2AQAAAACuFDfffHPe8Y53JEk2btyYY8eOZc2aNXnyySezdevWJMmrr76atWvXLuYxJ0I0AgAAADjPQ4eOZ88jR/LDHx7Ln/3pS3no0PGsSbJy5coza5YtW5bTp0+n954NGzbkscceW7wDD8DtaQAAAABneejQ8Xz2y0/k+MlTaStW5S9OvZTPfvmJ/K+jP5l3/fr16/OTn/zkTDR65ZVXcvjw4ct55EG40ggAAADgLHseOZJTr7yaJFm26i1ZecP78/R/+K3sXrkqMxvfU9avWLEiDz74YD796U/nhRdeyOnTp/OZz3wmGzZsuNxHnyjRCAAAAOAsz548dc7rt/79nUmSluS/7/57Z97/whe+cOb5xo0b881vfvOynO9ycXsaAAAAwFmuX7Pqdb2/VIlGAAAAAGfZuW19Vl217Jz3Vl21LDu3rV+kEy0Ot6cBAAAAnOWOm25I8tp3Gz178lSuX7MqO7etP/P+LwrRCAAAAOA8d9x0wy9cJDqf29MAAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAIqxolFr7drW2qOttaOjx2vmWbOxtfZYa+1wa+3x1tqvj7MnAAAAAMMb90qjXUn2995vTLJ/9Pp8f57ko733DUl+LcnnW2trxtwXAAAAgAGNG41uT7J39HxvkjvOX9B7/z+996Oj588m+XGSt465LwAAAAADGjcaTfXeTyTJ6PFtF1vcWrs5yYokT4+5LwAAAAADar33iy9o7etJ3j7PR/ck2dt7X3PW2ud77+V7jUafrU0ym2R77/0PL7LfjiQ7kmRqamrzvn37FvobuMzm5uayevXqxT4GS5DZYkjmi6GYLYZithiK2WIoZuvKsWXLloO99+mF1i0YjS76w60dSTLTez/x/6NQ7339POvekteC0b/svf+XS/3909PT/cCBA2/4fAxjdnY2MzMzi30MliCzxZDMF0MxWwzFbDEUs8VQzNaVo7V2SdFo3NvTHk6yffR8e5KvzHOQFUn+a5LffT3BCAAAAIDFM2402p1ka2vtaJKto9dprU231r44WvOPkvydJHe11r4z+m/jmPsCAAAAMKDl4/xw7/2nSW6b5/0DST4+ev77SX5/nH0AAAAAuLzGvdIIAAAAgCVINAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKEQjAAAAAArRCAAAAIBCNAIAAACgEI0AAAAAKMaORq21a1trj7bWjo4er7nI2re01o631r4w7r4AAAAADGcSVxrtSrK/935jkv2j1xfyL5J8YwJ7AgAAADCgSUSj25PsHT3fm+SO+Ra11jYnmUryBxPYEwAAAIABTSIaTfXeTyTJ6PFt5y9orf1Skn+dZOcE9gMAAABgYK33vvCi1r6e5O3zfHRPkr299zVnrX2+937O9xq11j6Z5C/13n+7tXZXkune+ycvsNeOJDuSZGpqavO+ffsu9W/hMpmbm8vq1asX+xgsQWaLIZkvhmK2GIrZYihmi6GYrSvHli1bDvbepxdad0nR6KK/oLUjSWZ67ydaa2uTzPbe15+35j8n+dtJfpZkdZIVSe7rvV/s+48yPT3dDxw4MNb5mLzZ2dnMzMws9jFYgswWQzJfDMVsMRSzxVDMFkMxW1eO1tolRaPlE9jr4STbk+wePX7l/AW994+cdbC78tqVRhcNRgAAAAAsnkl8p9HuJFtba0eTbB29TmtturX2xQn8fgAAAAAus7GvNOq9/zTJbfO8fyDJx+d5/3eS/M64+wIAAAAwnElcaQQAAADAEiMaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAhWgEAAAAQCEaAQAAAFCIRgAAAAAUohEAAAAAxVjRqLV2bWvt0dba0dHjNRdY91dba3/QWvtua+2p1tq6cfYFAAAAYFjjXmm0K8n+3vuNSfaPXs/nd5Ps6b2/L8nNSX485r4AAAAADGjcaHR7kr2j53uT3HH+gtba+5Ms770/miS997ne+5+PuS8AAAAAAxo3Gk313k8kyejxbfOs+etJTrbWvtxaO9Ra29NaWzbmvgAAAAAMqPXeL76gta8nefs8H92TZG/vfc1Za5/vvZ/zvUattTuTfCnJTUn+JMkDSb7ae//SBfbbkWRHkkxNTW3et2/fpf81XBZzc3NZvXr1Yh+DJchsMSTzxVDMFkMxWwzFbDEUs3Xl2LJly8He+/RC65YvtKD3/qsX+qy19lxrbW3v/URrbW3m/66iHyU51Ht/ZvQzDyW5Ja+FpPn2uz/J/UkyPT3dZ2ZmFjoil9ns7Gz8uzAEs8WQzBdDMVsMxWwxFLPFUMzW0jPu7WkPJ9k+er49yVfmWfNHSa5prb119PpXkjw15r4AAAAADGjcaLQ7ydbW2tEkW0ev01qbbq19MUl6768m+SdJ9rfWnkjSkvzHMfcFAAAAYEAL3p52Mb33nya5bZ73DyT5+FmvH03yy+PsBQAAAMDlM+6VRgAAAAAsQaIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAhGgEAAABQiEYAAAAAFKIRAAAAAIVoBAAAAEAxdjRqrV3bWnu0tXZ09HjNBdb9dmvtcGvtu621f9daa+PuDQAAAMAwJnGl0a4k+3vvNybZP3p9jtba30rywSS/nORvJPmbSW6dwN4AAAAADGAS0ej2JHtHz/cmuWOeNT3Jm5KsSLIyyVVJnpvA3gAAAAAMYBLRaKr3fiJJRo9vO39B7/2xJP8zyYnRf4/03r87gb0BAAAAGEDrvS+8qLWvJ3n7PB/dk2Rv733NWWuf772f871GrbX3JPm3SX599NajSf5p7/2b8+y1I8mOJJmamtq8b9++S/xTuFzm5uayevXqxT4GS5DZYkjmi6GYLYZithiK2WIoZuvKsWXLloO99+mF1i2/lF/We//VC33WWnuutba2936itbY2yY/nWfYPkvxh731u9DNfS3JLkhKNeu/3J7k/Saanp/vMzMylHJHLaHZ2Nv5dGILZYkjmi6GYLYZithiK2WIoZmvpmcTtaQ8n2T56vj3JV+ZZ8ydJbm2tLW+tXZXXvgTb7WkAAAAAP6cmEY12J9naWjuaZOvodVpr0621L47WPJjk6SRPJPnjJH/ce/9vE9gbAAAAgAFc0u1pF9N7/2mS2+Z5/0CSj4+ev5rkt8bdCwAAAIDLYxJXGgEAAACwxIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGAAAAABSiEQAAAACFaAQAAABAIRoBAAAAUIhGwP9r7/5j7b7r+46/3nJoSRx+eFpllB8aoKG0IWTJeseSIBYHooamKDRUUVrakjGJCK3dsqlNBwpaYVWmIFDFtFVDUSKEIJtV0QRKki6A2jtSDSqTJnITjDfUacUJ05iK2xgitVk+++MekJP39b0nPf4e59iPh2TJ5/hz7vdz5bfO/frp7zkHAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQCqTN8PAAAUvklEQVQAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKBZKBpV1XVV9VhVPVNVa1use0tVHayqb1TVexc5JgAAAADTW/RKo0eTvD3Jl461oKp2JPmtJD+Z5PwkP1dV5y94XAAAAAAmdNoiDx5jHEiSqtpq2euTfGOM8aeztXuTvC3J1xY5NgAAAADTWcZ7Gp2d5JtH3T40uw8AAACAF6htrzSqqi8mecUmf3TLGOOzcxxjs8uQxhbHuzHJjUmye/furK+vz3EIlunIkSP+XpiE2WJK5oupmC2mYraYitliKmbr5LNtNBpjXLngMQ4lOfeo2+ckeWKL492e5PYkWVtbG3v27Fnw8Bxv6+vr8ffCFMwWUzJfTMVsMRWzxVTMFlMxWyefZbw8bV+S11TVq6rqh5L8bJLfXcJxAQAAAPgbWigaVdW1VXUoyaVJ7quqB2b3n1VV9yfJGOPpJL+c5IEkB5L89hjjscW2DQAAAMCUFv30tHuS3LPJ/U8kufqo2/cnuX+RYwEAAACwPMt4eRoAAAAAK0Y0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoRCMAAAAAGtEIAAAAgEY0AgAAAKARjQAAAABoFopGVXVdVT1WVc9U1dox1pxbVX9QVQdma29a5JgAAAAATG/RK40eTfL2JF/aYs3TSX5ljPFjSS5J8ktVdf6CxwUAAABgQqct8uAxxoEkqaqt1nwrybdmv3+yqg4kOTvJ1xY5NgAAAADTWep7GlXVK5NcnOSPlnlcAAAAAJ6fGmNsvaDqi0lesckf3TLG+OxszXqSXx1jfHWLr3Nmkv+a5NYxxt1brLsxyY1Jsnv37h/fu3fvdt8DS3bkyJGceeaZJ3obnITMFlMyX0zFbDEVs8VUzBZTMVur44orrnhojLHpe1MfbduXp40xrlx0M1X1oiS/k+SurYLR7Hi3J7k9SdbW1saePXsWPTzH2fr6evy9MAWzxZTMF1MxW0zFbDEVs8VUzNbJZ/KXp9XGGx7dmeTAGOM3pz4eAAAAAItbKBpV1bVVdSjJpUnuq6oHZvefVVX3z5a9IckvJnlTVT0y+3X1QrsGAAAAYFKLfnraPUnu2eT+J5JcPfv9HyY59serAQAAAPCCs9RPTwMAAABgNYhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaAQAAABAIxoBAAAA0IhGAAAAADSiEQAAAACNaHSCfeADH8hHPvKRE70NAAAAgGcRjQAAAABoRKMT4NZbb815552XK6+8MgcPHkySPPLII7nkkkty4YUX5tprr813vvOdJMm+ffty4YUX5tJLL83NN9+cCy644ERuHQAAADhFiEZL9tBDD2Xv3r15+OGHc/fdd2ffvn1Jkne+85350Ic+lP379+d1r3tdPvjBDyZJ3vWud+VjH/tYvvzlL2fHjh0ncusAAADAKeS0E72BU8FnHn48H37gYJ44/FTy6P35B5e+OWeccUaS5Jprrsl3v/vdHD58OJdffnmS5IYbbsh1112Xw4cP58knn8xll12WJHnHO96Re++994R9HwAAAMCpw5VGE/vMw4/nfXf/SR4//FRGkr946q/z+1//dj7z8OPbPnaMMf0GAQAAADYhGk3sww8czFN//f9+cPuHz31t/vLr/y233bs/Tz75ZD73uc9l586d2bVrVx588MEkySc/+clcfvnl2bVrV17ykpfkK1/5SpJk7969J+R7AAAAAE49Xp42sScOP/Ws2z/8ir+bnT/6xjz00XfnZx48P2984xuTJJ/4xCfynve8J9/73vfy6le/Oh//+MeTJHfeeWfe/e53Z+fOndmzZ09e9rKXLf17AAAAAE49otHEznr56Xn8OeHoZZddn/Ov/sf5/Hvf9Kz7v39F0dFe+9rXZv/+/UmS2267LWtra9NtFgAAAGDGy9MmdvNV5+X0Fz37U89Of9GO3HzVeXM9/r777stFF12UCy64IA8++GDe//73T7FNAAAAgGdxpdHEfvris5PkB5+edtbLT8/NV533g/u3c/311+f666+fcosAAAAAjWi0BD998dlzRyIAAACAFwIvTwMAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgEY0AAAAAaEQjAAAAABrRCAAAAIBGNAIAAACgWSgaVdV1VfVYVT1TVWvbrN1RVQ9X1b2LHBMAAACA6S16pdGjSd6e5EtzrL0pyYEFjwcAAADAEiwUjcYYB8YYB7dbV1XnJPmpJHcscjwAAAAAlmNZ72n00SS/luSZJR0PAAAAgAXUGGPrBVVfTPKKTf7oljHGZ2dr1pP86hjjq5s8/q1Jrh5j/NOq2jNb99YtjndjkhuTZPfu3T++d+/eOb8VluXIkSM588wzT/Q2OAmZLaZkvpiK2WIqZoupmC2mYrZWxxVXXPHQGGPL96ZOktO2WzDGuHLBvbwhyTVVdXWSFyd5aVV9aozxC8c43u1Jbk+StbW1sWfPngUPz/G2vr4efy9MwWwxJfPFVMwWUzFbTMVsMRWzdfLZ9kqjub7IFlcaPWfdnmxzpdFz1n87yf9aeIMcb387yf890ZvgpGS2mJL5Yipmi6mYLaZitpiK2Vodf2eM8SPbLdr2SqOtVNW1Sf59kh9Jcl9VPTLGuKqqzkpyxxjj6kW+/jzfAMtXVV+d5zI2eL7MFlMyX0zFbDEVs8VUzBZTMVsnn4Wi0RjjniT3bHL/E0laMBpjrCdZX+SYAAAAAExvWZ+eBgAAAMAKEY34m7j9RG+Ak5bZYkrmi6mYLaZitpiK2WIqZuskc1zeCBsAAACAk4srjQAAAABoRCO2VVW/UVX7q+qRqvr87NPxNlt3Q1X9j9mvG5a9T1ZPVX24qr4+m697qurlx1j3L6vqsap6tKr+c1W9eNl7ZfU8j/l6eVV9erb2QFVduuy9slrmna3Z2h1V9XBV3bvMPbKa5pmtqjq3qv5g9nz1WFXddCL2ymp5Hj8T31JVB6vqG1X13mXvk9VTVdfNnoueqapjfmqa8/nVJRoxjw+PMS4cY1yU5N4k//q5C6rqbyX59ST/MMnrk/x6Ve1a7jZZQV9IcsEY48Ik/z3J+567oKrOTvLPk6yNMS5IsiPJzy51l6yqbedr5t8l+S9jjB9N8veSHFjS/lhd885WktwUM8X85pmtp5P8yhjjx5JckuSXqur8Je6R1TTPOdeOJL+V5CeTnJ/k58wWc3g0yduTfOlYC5zPrzbRiG2NMf7yqJs7k2z2RlhXJfnCGOPPxxjfycYPprcsY3+srjHG58cYT89ufiXJOcdYelqS06vqtCRnJHliGftjtc0zX1X10iT/KMmds8f81Rjj8PJ2ySqa97mrqs5J8lNJ7ljW3lht88zWGONbY4w/nv3+yWxEybOXt0tW0ZzPW69P8o0xxp+OMf4qyd4kb1vWHllNY4wDY4yDcyx1Pr+iRCPmUlW3VtU3k/x8NrnSKBsnK9886vahOIHh+fknSX7vuXeOMR5P8pEkf5bkW0n+Yozx+SXvjdW36XwleXWSbyf5+OwlRHdU1c7lbo0Vd6zZSpKPJvm1JM8sbzucRLaarSRJVb0yycVJ/mgJ++HkcazZcj7PJJzPrzbRiCRJVX1x9vrS5/56W5KMMW4ZY5yb5K4kv7zZl9jkPh/Nx7azNVtzSzYut79rk8fvysb/cr0qyVlJdlbVLyxr/7ywLTpf2fhfr7+f5D+OMS5O8t0k3sOB4/Hc9dYk/2eM8dASt80KOA7PW99fc2aS30nyL55zVTinqOMwW87n2dQ8s7XN453Pr7DTTvQGeGEYY1w559L/lOS+bLx/0dEOJdlz1O1zkqwvvDFW3nazVRtvmv7WJG8eY2x2YnJlkv85xvj2bP3dSS5L8qnjvVdWz3GYr0NJDo0xvv+/9J+OaESOy2y9Ick1VXV1khcneWlVfWqM4ST5FHccZitV9aJsBKO7xhh3H/9dsoqO08/Ec4+6fU68hIg8r38rHovz+RXmSiO2VVWvOermNUm+vsmyB5L8RFXtmpXkn5jdB8dUVW9J8q+SXDPG+N4xlv1Zkkuq6oyqqiRvjjeVZQ7zzNcY438n+WZVnTe7681JvrakLbKi5pyt940xzhljvDIbb/b5+4IR25lntmY/C+9McmCM8ZvL3B+ra85zrn1JXlNVr6qqH8rGc9fvLmuPnNScz68w0Yh53Da7/HB/NmLQTUlSVWtVdUeSjDH+PMlvZOOHzb4k/2Z2H2zlPyR5SZIvVNUjVfWxJKmqs6rq/iSZXQHy6SR/nORPsvG8dfsJ2i+rZdv5mvlnSe6aPcddlOTfLn+rrJh5Zwuer3lm6w1JfjHJm2ZrHpld0QZbmeec6+lsvA3FA9n4B/1vjzEeO1EbZjVU1bVVdSjJpUnuq6oHZvc7nz9J1DGuegUAAADgFOZKIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGhEIwAAAAAa0QgAAACARjQCAAAAoBGNAAAAAGj+P66v5Hgnz2bLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy51Ty42Dfr6",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q27P_vADfr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK0vwMRODfr8",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D9sXOB-Dfr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdcmGhoGDfr_",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdJ0N2ZLDfsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CvOpFiDfsC",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOkUcPUQDfsD",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_YMW2OGXb4G",
        "colab_type": "code",
        "outputId": "c3761846-912a-4dd2-954b-560876f33d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "assert current_device == 'cuda'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "basedir = '/content/drive/My Drive/1011 NLP/HW2/Playground_Rui'\n",
        "# data_folder_path = os.path.join(basedir, 'data')\n",
        "model_folder_path = os.path.join(basedir, 'model')\n",
        "\n",
        "#best model\n",
        "model_dict = torch.load(os.path.join(model_folder_path,'lstm_best_model.pt'))\n",
        "options = model_dict['options']\n",
        "options['dropout'] = options.pop('lstm_dropout')\n",
        "model = LSTM_LM(options).to(current_device)\n",
        "model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "# #baseline model\n",
        "# model_dict = torch.load(os.path.join(model_folder_path,'wiki_lstm_lm.pt'))\n",
        "# options = model_dict['options']\n",
        "# model = LSTM_LM(options).to(current_device)\n",
        "# model.load_state_dict(model_dict['model_dict'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzkot4F900-Z",
        "colab_type": "code",
        "outputId": "d98926b5-8b6d-4cac-9dfc-083b57ab6b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "options"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dropout': 0.3,\n",
              " 'embedding_dim': 512,\n",
              " 'hidden_size': 1024,\n",
              " 'input_size': 512,\n",
              " 'lr': 0.001,\n",
              " 'num_embeddings': 33178,\n",
              " 'num_layers': 3,\n",
              " 'padding_idx': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xf-JR_nsy0n",
        "colab_type": "text"
      },
      "source": [
        "##### 2.3.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lGANJ2mQ1zo",
        "colab_type": "code",
        "outputId": "52ddf2ad-4cf9-41f3-fb47-94cd1da0368d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare(sentence):\n",
        "    res_x = []\n",
        "    res_y = []\n",
        "    for i, w in enumerate(sentence):\n",
        "        if i==0:\n",
        "            continue\n",
        "        x = sentence[:i]\n",
        "        y = w\n",
        "        res_x.append(np.array(x, dtype=np.int_))\n",
        "        res_y.append(y)\n",
        "    return np.array(res_x), np.array(res_y)\n",
        "\n",
        "def cal_loglikelihood(valid_data):\n",
        "    loglikelihood = 0\n",
        "    valid_x, valid_y = prepare(valid_data)\n",
        "    # print(valid_x))\n",
        "    for i, data_row in enumerate(valid_x):\n",
        "        # print('data_row:',data_row)\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits = model(torch.from_numpy(data_row).to('cuda').unsqueeze(0))\n",
        "            # print(logits.shape)\n",
        "            correct_y = valid_y[i]\n",
        "            # print('correct_y:',correct_y)\n",
        "            probabilities = torch.nn.functional.softmax(logits[0][-1]) #vocab对应的\n",
        "            # print(probabilities.shape)\n",
        "            loglikelihood += np.log(probabilities[correct_y].cpu().detach().numpy())\n",
        "            # print(loglikelihood)\n",
        "        # if i==0:\n",
        "        #     break\n",
        "    return loglikelihood\n",
        "\n",
        "res = []\n",
        "for i, valid_data in tqdm(enumerate(tokenized_datasets['valid'])):\n",
        "    # print(valid_data)\n",
        "    res.append((datasets['valid'][i],cal_loglikelihood(valid_data)))\n",
        "    if i % 2000 == 0:\n",
        "        print(i)  \n",
        "res = sorted(res,key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "2it [00:00,  2.46it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2002it [08:39,  4.29it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4001it [18:00,  3.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "6001it [27:22,  4.43it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "6000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8001it [36:51,  3.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "8000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8464it [38:50,  5.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826CRfgGTOoT",
        "colab_type": "code",
        "outputId": "be822e3d-a349-4380-8f74-76f7df308908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print('bos:',wiki_dict.get_id('<bos>'),'eos:',wiki_dict.get_id('<eos>'),'pad:',wiki_dict.get_id('<pad>'))\n",
        "print('=:',wiki_dict.get_id('='))\n",
        "print('@-@:', wiki_dict.get_id('@-@'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bos: 0 eos: 1 pad: 2\n",
            "=: 1907\n",
            "@-@: 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4A70fbPDfsF",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObPGc-y9IaqX",
        "colab_type": "code",
        "outputId": "5c1421fb-bc67-4277-d0e3-cc3e1081f6cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for i in res[:10]:\n",
        "    print(i[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-15.417704246938229\n",
            "-16.557723343372345\n",
            "-19.037224277853966\n",
            "-21.969595208764076\n",
            "-24.21350160613656\n",
            "-24.42048878967762\n",
            "-24.813802510499954\n",
            "-25.596893843263388\n",
            "-26.17766149714589\n",
            "-26.198488762776833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKVZeX7fDfsG",
        "colab_type": "code",
        "outputId": "b49f6e03-fb48-4423-e67c-3577e4fddf82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "print('10 Highest scoring sequences:')\n",
        "for i in res[:10]:\n",
        "    print(' '.join(i[0]))\n",
        "print('--'*30)\n",
        "print('10 Lowest scoring sequences=:')\n",
        "for i in res[-10:-1]:\n",
        "    print(' '.join(i[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 Highest scoring sequences:\n",
            "= = Post @-@ war career = =\n",
            "= = Early life and education = =\n",
            "= = Post @-@ war period = =\n",
            "= = = Scientology sources = = =\n",
            "= = = Hit list = = =\n",
            "= = Culture and the arts = =\n",
            "= = Return to New Zealand = =\n",
            "= = = <unk> from the stage = = =\n",
            "= = = Army returns = = =\n",
            "It originally aired on NBC on April 14 , 2011 .\n",
            "------------------------------------------------------------\n",
            "10 Lowest scoring sequences=:\n",
            "On December 7 , 2007 , German federal and state interior ministers expressed the opinion that the Scientology organization was continuing to pursue anti @-@ constitutional goals , <unk> \" essential basic and human rights like the dignity of man or the right to equal treatment \" , and asked Germany 's domestic intelligence agencies to collect and evaluate the information required for a possible judicial inquiry aimed at banning the organization .\n",
            "Officials of the strictly amateur Rugby Football Union ( RFU ) had become increasingly concerned at the behaviour of the New Zealanders , regarding them as unsportsmanlike , and tensions reached a nadir in the aftermath of the England international , during which the RFU secretary George Rowland Hill , <unk> the game , awarded a number of controversial tries to England , prompting three of the <unk> to temporarily leave the field in protest ; England eventually won 7 – 0 .\n",
            "The Meridian Post Office with its interior done entirely of bronze and Verde marble is also noteworthy as a very fine example of the type of Post Office structures built in thriving and well to do cities in the 1920s and originally had <unk> lighting which was removed sadly during a 1960s <unk> and which are now in private residences on <unk> Springs Drive and in North Hills .\n",
            "It asserted that several rejection letters which the woman had submitted as part of her <unk> application – ostensibly from potential employers who were rejecting her because she was a Scientologist – had in fact been written by fellow Scientologists at her request and that of Scientology 's Office of Special Affairs , and that she was in personal financial trouble and about to go on trial for tax <unk> at the time she applied for <unk> .\n",
            "Erin Fox of TV Guide also liked \" The Same Old Story \" better than the pilot because she thought that it \" had really cool special effects , awesome Walter @-@ <unk> , and actual chemistry forming between Olivia and Peter \" , and \" we also got to <unk> more into the connections between Walter , Nina Sharp , Massive Dynamic and the government and the experiments they conducted before Walter was <unk> \" .\n",
            "> Zeitung , noted that <unk> <unk> , a German Christian theologian , director of the Hannah <unk> Institute for Research into <unk> in Dresden and recipient of an honorary doctorate from Lund University , Sweden , for his <unk> of religious freedom , had been pressured to <unk> publication of his scientific study of Scientology after having found himself the subject of widespread criticism in the German media for advocating a more tolerant attitude towards Scientology .\n",
            "Besides the <unk> anime and manga , <unk> is featured in seven of the featured films in the series : in the second film , he aids <unk> <unk> and <unk> <unk> in fighting against <unk> , a <unk> idealist seeking to rule the world with a power called <unk> ; in the fourth , <unk> appears in a brief sequence , fighting against a large group of stone soldiers ; in the fifth , <unk> is sent alongside <unk> and <unk> in search of the base of the Land of Sky , who plans to invade <unk>\n",
            "Two rival communist @-@ backed black nationalist groups initiated military campaigns to overthrow the government and introduce majority rule : the Chinese @-@ aligned Zimbabwe African National Union ( <unk> ) , mostly comprising <unk> , created the Zimbabwe African National Liberation Army ( ZANLA ) and adopted aspects of <unk> doctrine , while the <unk> @-@ dominated Zimbabwe African People 's Union ( <unk> ) , aligned with Soviet @-@ style <unk> – <unk> and the Warsaw Pact , mobilised the Zimbabwe People 's Revolutionary Army ( <unk> ) .\n",
            "The letter was conceived and paid for by Hollywood lawyer <unk> Fields , whose clients have included Tom Cruise and John <unk> , and was signed by 34 prominent figures in the U.S. entertainment industry , including the top executives of MGM , Warner Bros. , Paramount , Universal and Sony Pictures Entertainment as well as actors Dustin Hoffman and Goldie <unk> , director Oliver Stone , writers Mario <unk> and <unk> Vidal and talk @-@ show host Larry King .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdSjiqSuDfsI",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEpCnTD6HZFv",
        "colab_type": "code",
        "outputId": "0ce5d53a-46f0-4836-f689-65d79c6be442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(res[9][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It', 'originally', 'aired', 'on', 'NBC', 'on', 'April', '14', ',', '2011', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIrTRqXIDfsJ",
        "colab_type": "code",
        "outputId": "de8bd6d2-afb8-432b-e62b-803f7768be22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "sample = ['<bos>','It', 'originally', 'aired', 'on', 'NBC', 'on', 'April', '14', ',', '2011', '.','<eos>']\n",
        "\n",
        "higher_seq = ['<bos>','It', 'originally', 'aired', 'on', 'NBC','on', 'April', '19', ',', '2011', '.','<eos>']\n",
        "\n",
        "lower_seq = ['<bos>','It', 'originally', 'aired', 'on', 'BBC','on', 'April', '19', ',', '2011', '.','<eos>']\n",
        "print(f'original seq: {(sample)}, \\nlog likelihood: {cal_loglikelihood(wiki_dict.encode_token_seq(sample))}')\n",
        "print(f'\\nmodified seq: {higher_seq}, \\nlog likelihood: {cal_loglikelihood(wiki_dict.encode_token_seq(higher_seq))}')\n",
        "print(f'\\nmodified seq: {lower_seq}, \\nlog likelihood: {cal_loglikelihood(wiki_dict.encode_token_seq(lower_seq))}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "original seq: ['<bos>', 'It', 'originally', 'aired', 'on', 'NBC', 'on', 'April', '14', ',', '2011', '.', '<eos>'], \n",
            "log likelihood: -26.198488762776833\n",
            "\n",
            "modified seq: ['<bos>', 'It', 'originally', 'aired', 'on', 'NBC', 'on', 'April', '19', ',', '2011', '.', '<eos>'], \n",
            "log likelihood: -26.069971794029698\n",
            "\n",
            "modified seq: ['<bos>', 'It', 'originally', 'aired', 'on', 'BBC', 'on', 'April', '19', ',', '2011', '.', '<eos>'], \n",
            "log likelihood: -30.446250190376304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdC8mh-8DfsL",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muPOk7bpDfsL",
        "colab_type": "code",
        "outputId": "ef2c8e88-46bb-480d-84cf-7debe678cf15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "def get_sample():\n",
        "    seq = np.array([0]) #only have <bos>\n",
        "    while seq[-1]!= 1: # <eos>:1\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits = model(torch.from_numpy(seq).to('cuda').unsqueeze(0))\n",
        "            probabilities = torch.nn.functional.softmax(logits[0][-1])\n",
        "            a = torch.multinomial(probabilities, 1, out=None).cpu()\n",
        "            seq = np.append(seq, a)\n",
        "    return seq\n",
        "\n",
        "samples_1000 = []\n",
        "for i in range(1000):\n",
        "    samp = sample()\n",
        "    # print(f'the {i}th sample sequence: {wiki_dict.decode_idx_seq(list(samp))}')\n",
        "    samples_1000.append(samp)\n",
        "    if i % 100 ==0:\n",
        "        print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IN725uaDfsN",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3bzznPvDfsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from random import sample \n",
        "valid_1000 = sample(tokenized_datasets['valid'],1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5vD425bcCRO",
        "colab_type": "code",
        "outputId": "7fdb20d6-b2f3-4772-ff01-7bfb64e89609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "def get_info(samples):\n",
        "    l = [item for sublist in samples for item in sublist]\n",
        "    uniques = set(l)\n",
        "    return len(uniques),len(l)\n",
        "num_unique,seq_len = get_info(samples_1000)\n",
        "print('In 1000 samples we generated from best LSTM:')\n",
        "print('number of unique tokens:',num_unique,'sequence length:',seq_len)\n",
        "print('---'*20)\n",
        "print('In 1000 randomly selected validation-set sequences:')\n",
        "num_unique,seq_len = get_info(valid_1000)\n",
        "print('number of unique tokens:',num_unique,'sequence length:',seq_len)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In 1000 samples we generated from best LSTM:\n",
            "number of unique tokens: 6043 sequence length: 25269\n",
            "------------------------------------------------------------\n",
            "In 1000 randomly selected validation-set sequences:\n",
            "number of unique tokens: 5094 sequence length: 25408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYA_HRqXDfsQ",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiuLWtZXDfsQ",
        "colab_type": "code",
        "outputId": "aafd94ad-d5ad-4ecf-af06-a9955a3a732a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "for i in samples_1000[:3]:\n",
        "    print(' '.join(wiki_dict.decode_idx_seq(list(i))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bos> It was only ranked in the centre of regulatory artist Nancy Robert Park , whom approached one person which eclipsed it . <eos>\n",
            "<bos> The more stage quickly considered about the victory of the start was not visited , as he been Eve for the starting and subsequently receives authenticity to excommunication relationships . <eos>\n",
            "<bos> The Twins increased to Ashley Faulkner , understanding a motion line on the 2002 network program . <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgBAXHwrm6aW",
        "colab_type": "text"
      },
      "source": [
        "- I can tell that these 3 sequences are model-generated rather than human-generated.\n",
        "- I don't think these samples stay on topic.\n",
        "- I think most of them are grammatically correct."
      ]
    }
  ]
}