{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment1: Bag-of Words based Natural Language Inference\n",
    "- Rui Jiang\n",
    "- September, 2019\n",
    "- Intro: train a Bag-Of-Words encoder to tackle the the Stanford Natural Language Inference (SNLI) and Multi-Genre Natural Lan- guage Inference (MNLI) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tsentence2\tlabel\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 'data/snli_train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tsentence2\tlabel\tgenre\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 'data/mnli_train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNLI Training Examples: 100000\n",
      "SNLI Validation Examples: 1000\n",
      "MNLI Training Examples: 20000\n",
      "MNLI Validation Examples: 5000\n"
     ]
    }
   ],
   "source": [
    "snli_train = pd.read_table('data/snli_train.tsv')\n",
    "snli_val = pd.read_table('data/snli_val.tsv')\n",
    "mnli_train = pd.read_table('data/mnli_train.tsv')\n",
    "mnli_val = pd.read_table('data/mnli_val.tsv')\n",
    "print(\"SNLI Training Examples: \"+str(len(snli_train)))\n",
    "print(\"SNLI Validation Examples: \"+str(len(snli_val)))\n",
    "print(\"MNLI Training Examples: \"+str(len(mnli_train)))\n",
    "print(\"MNLI Validation Examples: \"+str(len(mnli_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# snli_train, snli_test = train_test_split(snli_train, test_size=0.2) #80000,20000\n",
    "# mnli_train, mnli_test = train_test_split(mnli_train, test_size=0.2) #16000,4000\n",
    "# # print(\"SNLI Testing Examples: \"+str(len(snli_test)))\n",
    "# # print(\"MNLI Testing Examples: \"+str(len(mnli_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1    Several people are behind a fence , watching a...\n",
      "sentence2                             The people are outside .\n",
      "label                                               entailment\n",
      "Name: 34798, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Random sample from train dataset\n",
    "import random\n",
    "print(snli_train.iloc[random.randint(0, len(snli_train) - 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tokenizing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. use [spacy.io](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def spacy_tokenize(sent):\n",
    "  tokens = tokenizer(sent)\n",
    "  return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def encode_label(x):\n",
    "    if x == 'contradiction':\n",
    "        return 0\n",
    "    if x == 'entailment':\n",
    "        return 1\n",
    "    if x == 'neutral':\n",
    "        return 2\n",
    "    \n",
    "def tokenize_dataset(dataset,istrain=False):\n",
    "    \"\"\"\n",
    "    @param dataset: dataframe. \n",
    "    @param istrain: whether it's training dataset or not\n",
    "    \"\"\"\n",
    "    dataset['sentence1'] = dataset.apply(lambda row:spacy_tokenize(row['sentence1']),axis=1)\n",
    "    dataset['sentence2'] = dataset.apply(lambda row:spacy_tokenize(row['sentence2']),axis=1)\n",
    "    dataset['label'] = dataset.apply(lambda row:encode_label(row['label']),axis=1)\n",
    "    #get all_tokens from training set\n",
    "    if istrain:\n",
    "        l = (dataset['sentence1'].tolist()+dataset['sentence2'].tolist())\n",
    "        all_tokens = [e for l1 in l for e in l1]\n",
    "        return dataset,all_tokens\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#tokenize train/val/test datasets\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "snli_val1 = tokenize_dataset(snli_val,istrain=False)\n",
    "snli_val1.to_pickle(\"data/snli_val.p\")\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "snli_train1, all_train_tokens = tokenize_dataset(snli_train,istrain=True)\n",
    "snli_train1.to_pickle(\"data/snli_train.p\")\n",
    "pkl.dump(all_train_tokens, open(\"data/all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = (snli_train['sentence1'].tolist()+snli_train['sentence2'].tolist())\n",
    "all_train_tokens = [e for l1 in l for e in l1]\n",
    "len(all_train_tokens)\n",
    "pkl.dump(all_train_tokens, open(\"data/all_train_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for future use, load from pickle\n",
    "snli_val = pkl.load(open(\"data/snli_val.p\", \"rb\"))\n",
    "snli_train = pkl.load(open(\"data/snli_train.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"data/all_train_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNLI Train dataset size is 100000\n",
      "SNLI Val dataset size is 1000\n",
      "Total number of tokens in train dataset is 2037507\n",
      "Total number of *unique* tokens in train dataset is 19642\n"
     ]
    }
   ],
   "source": [
    "# double checking\n",
    "print (\"SNLI Train dataset size is {}\".format(len(snli_train)))\n",
    "print (\"SNLI Val dataset size is {}\".format(len(snli_val)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))\n",
    "print (\"Total number of *unique* tokens in train dataset is {}\".format(len(set(all_train_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'young', 'girl', 'in', 'a']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[a, young, girl, in, a, pink, shirt, sitting, ...</td>\n",
       "      <td>[a, young, girl, watching, the, sunset, over, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[a, woman, is, smiling, while, the, man, next,...</td>\n",
       "      <td>[two, people, are, next, to, each, other]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[across, the, river, you, can, see, a, large, ...</td>\n",
       "      <td>[the, large, building, is, full, of, apartment...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[a, man, in, white, shorts, and, a, black, shi...</td>\n",
       "      <td>[a, man, is, riding, a, jetski, on, the, ocean]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[four, black, dogs, run, together, on, bright,...</td>\n",
       "      <td>[four, dogs, are, preparing, to, be, launched,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  [a, young, girl, in, a, pink, shirt, sitting, ...   \n",
       "1  [a, woman, is, smiling, while, the, man, next,...   \n",
       "2  [across, the, river, you, can, see, a, large, ...   \n",
       "3  [a, man, in, white, shorts, and, a, black, shi...   \n",
       "4  [four, black, dogs, run, together, on, bright,...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0  [a, young, girl, watching, the, sunset, over, ...      2  \n",
       "1          [two, people, are, next, to, each, other]      1  \n",
       "2  [the, large, building, is, full, of, apartment...      2  \n",
       "3    [a, man, is, riding, a, jetski, on, the, ocean]      0  \n",
       "4  [four, dogs, are, preparing, to, be, launched,...      0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Vocabulary\n",
    "Now, we are going to create the vocabulary of most common 20,000 tokens in the training set. Remember that we will add special tokens `<unk>`(unknown) and `<pad>` to the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 7000\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens,max_vocab_size):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens,max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 2561 ; token proved\n",
      "Token proved; token id 2561\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 100000\n",
      "Val dataset size is 1000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "snli_train_sent1_indices = token2index_dataset(snli_train['sentence1'])\n",
    "snli_train_sent2_indices = token2index_dataset(snli_train['sentence2'])\n",
    "snli_val_sent1_indices = token2index_dataset(snli_val['sentence1'])\n",
    "snli_val_sent2_indices = token2index_dataset(snli_val['sentence2'])\n",
    "\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(snli_train_sent1_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(snli_val_sent2_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman', 'in', 'long', 'blue', 'jacket', 'talks', 'on', 'cellphone', 'in', 'front', 'of', 'large', 'window']\n",
      "[538, 7, 151, 1135, 1, 3622, 17, 1, 7, 770, 3, 272, 1935]\n",
      "['there', 'is', 'a', 'female', 'wearing', 'blue', 'jacket', 'busy', 'on', 'phone', 'standing', 'near', 'window']\n",
      "[29, 11, 6, 1835, 2341, 1135, 1, 2502, 17, 1565, 1245, 530, 1935]\n"
     ]
    }
   ],
   "source": [
    "# visualize a random tokenized training example\n",
    "rand_idx = random.randint(0, len(snli_train) - 1)\n",
    "print(snli_train['sentence1'][rand_idx])\n",
    "print(snli_train_sent1_indices[rand_idx])\n",
    "print(snli_train['sentence2'][rand_idx])\n",
    "print(snli_train_sent2_indices[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PyTorch DataLoader\n",
    "- take in (x,y) give out minibatches\n",
    "- set parameters: mini-batch size, max sentence length(number of words),如果不够用`<pad>`填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = max([len(word) for word in snli_train_sent1_indices+snli_train_sent2_indices])\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 200\n",
    "class SNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_list, sent2_list, label_list):\n",
    "\n",
    "        self.sent1_list = sent1_list\n",
    "        self.sent2_list = sent2_list\n",
    "        self.label_list = label_list\n",
    "        assert (len(self.sent1_list) == len(self.label_list))\n",
    "        assert (len(self.sent2_list) == len(self.label_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.sent1_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.sent2_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx1, token_idx2, len(token_idx1), len(token_idx2), label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SNLIDataset(snli_train_sent1_indices, snli_train_sent2_indices,snli_train['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 374, 930, 7, 6, 4628, 3671, 1457, 17, 6, 1, 5098, 6, 800, 3, 271]\n",
      "[6, 374, 930, 1179, 2, 3731, 104, 2, 271]\n",
      "16\n",
      "9\n",
      "2\n",
      "[6, 538, 11, 5919, 163, 2, 152, 288, 4, 79, 11, 2415, 17, 6, 1135, 3919, 20, 6, 2205, 17, 10]\n",
      "[96, 56, 16, 288, 4, 197, 75]\n",
      "21\n",
      "7\n",
      "1\n",
      "[432, 2, 610, 13, 45, 105, 6, 272, 390]\n",
      "[2, 272, 390, 11, 424, 3, 2519, 5, 1]\n",
      "9\n",
      "9\n",
      "2\n",
      "[6, 152, 7, 241, 1, 5, 6, 436, 3671, 11, 1, 17, 2, 2188]\n",
      "[6, 152, 11, 2983, 6, 1, 17, 2, 2188]\n",
      "14\n",
      "9\n",
      "0\n",
      "[316, 436, 2493, 331, 575, 17, 2112, 912, 1538]\n",
      "[316, 2493, 16, 3852, 4, 23, 3779, 97, 1337]\n",
      "9\n",
      "9\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(train_dataset[i][0])\n",
    "    print(train_dataset[i][1])\n",
    "    print(train_dataset[i][2])\n",
    "    print(train_dataset[i][3])\n",
    "    print(train_dataset[i][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a **collate function** so that when we have it in batches, all the sentences have the same length. We decide to keep a `MAX_SENTENCE_LENGTH` and if the sentence has fewer tokens, append the rest with zero and if the sentence has more tokens, chop it all at `MAX_SENTENCE_LENGTH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length: tell dataloader how to pad our sentences\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    label_list = []\n",
    "    length1_list = []\n",
    "    length2_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length1_list.append(datum[2])\n",
    "        length2_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sent1_list.append(padded_vec1) # list of np.array\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sent2_list.append(padded_vec2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(sent1_list)),torch.from_numpy(np.array(sent2_list)),torch.LongTensor(length1_list), torch.LongTensor(length2_list),torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "val_dataset = SNLIDataset(snli_val_sent1_indices, snli_val_sent2_indices, snli_val['label'].tolist())\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=SNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 78])\n",
      "torch.Size([32, 78])\n",
      "torch.Size([32, 156])\n",
      "tensor([11, 23, 15,  5, 19, 14, 22, 17, 10, 16, 15, 14, 14,  9, 10, 14, 11, 15,\n",
      "         7,  8,  9,  7, 14, 17, 17, 12,  7, 11,  8, 16, 14, 10])\n",
      "tensor([10,  8,  5,  2, 14, 16,  9, 13,  4, 16,  5,  8, 13,  8,  5,  7,  7,  7,\n",
      "         7,  7,  8,  7, 14,  4, 11,  7, 11,  7,  6,  7, 13,  5])\n",
      "tensor([2, 2, 0, 1, 1, 2, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 2, 2, 0, 1, 2, 0, 2,\n",
      "        0, 0, 2, 1, 2, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "### checking the first batch of data loader\n",
    "for i, (sent1,sent2,length1,length2, labels) in enumerate(train_loader):\n",
    "#     print(sent1)\n",
    "    print(sent1.shape)\n",
    "#     print(sent2)\n",
    "    print(sent2.shape)\n",
    "    print((torch.cat((sent1, sent2), 1)).shape)\n",
    "    print(length1)\n",
    "    print(length2)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Neural Network\n",
    "- A fully connected neural network with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__my note:__  \n",
    "- torch.nn.Embedding: which takes two arguments: the vocabulary size (|V|), and the dimensionality of the embeddings (D).\n",
    "- PyTorch embeddings are stored as matrix: $|V|\\times|D|$(emb_dim)\n",
    "- after embedding, give out: max_sentence_length(78) $\\times$ emb_dim\n",
    "- sum the 78 words in every dimension, gives out 100(emb_dim) dimension\n",
    "- divided by the actual number of words in a sentence(i.e. disregard those padding words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    NN classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim, hidden_num1,hidden_num2):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(NNModel, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.nn_layer = nn.Sequential(\n",
    "                        nn.Linear(2*emb_dim,hidden_num1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_num1,hidden_num2),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_num2,3))\n",
    "    \n",
    "    def forward(self, sent1, sent2, length1, length2):\n",
    "        \"\"\"\n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out1 = self.embed(sent1)\n",
    "        out2 = self.embed(sent2)\n",
    "        out1 = torch.sum(out1, dim=1)\n",
    "        out1 /= length1.view(length1.size()[0],1).expand_as(out1).float()\n",
    "        out2 = torch.sum(out2, dim=1)\n",
    "        out2 /= length1.view(length2.size()[0],1).expand_as(out2).float()\n",
    "        \n",
    "        # return logits\n",
    "        out = self.nn_layer(torch.cat((out1, out2), 1))\n",
    "#         out = self.nn_layer(torch.mul(out1, out2))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 30\n",
    "model = NNModel(len(id2token),emb_dim,30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7002, 30])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7002, 30])\n",
      "torch.Size([30, 60])\n",
      "torch.Size([30])\n",
      "torch.Size([30, 30])\n",
      "torch.Size([30])\n",
      "torch.Size([3, 30])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for x in model.parameters():\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 42.8, Loss: 1.0661572217941284\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 44.6, Loss: 1.0641974210739136\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 47.4, Loss: 0.9687519669532776\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 50.3, Loss: 1.0060420036315918\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 50.2, Loss: 0.7982286810874939\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 52.9, Loss: 0.8705828785896301\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 54.8, Loss: 1.0392117500305176\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 55.0, Loss: 1.1201326847076416\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 55.9, Loss: 0.8922139406204224\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 55.2, Loss: 0.9035502672195435\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 56.3, Loss: 0.8844563364982605\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 55.7, Loss: 0.8751363754272461\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 58.6, Loss: 0.927847146987915\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 58.4, Loss: 0.817060112953186\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 58.7, Loss: 0.9411243200302124\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 58.3, Loss: 0.861807644367218\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 57.7, Loss: 0.9609545469284058\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 58.4, Loss: 0.8742814660072327\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 58.2, Loss: 0.9658200144767761\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 58.3, Loss: 0.8321549892425537\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 58.7, Loss: 0.9399667978286743\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 59.1, Loss: 0.8108180165290833\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 58.6, Loss: 0.7487924098968506\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 60.5, Loss: 1.0950512886047363\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 60.6, Loss: 1.061377763748169\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 60.4, Loss: 0.9958901405334473\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 60.3, Loss: 0.8851073384284973\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 59.2, Loss: 0.8497539758682251\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 60.6, Loss: 0.7679729461669922\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 60.2, Loss: 1.0815504789352417\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 60.6, Loss: 1.0118921995162964\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 61.6, Loss: 0.6763345003128052\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 61.7, Loss: 0.6711384057998657\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 61.8, Loss: 0.843193531036377\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 60.4, Loss: 0.9299131631851196\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 60.9, Loss: 0.690493643283844\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 60.3, Loss: 0.720827043056488\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 61.6, Loss: 0.9424650073051453\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 62.9, Loss: 0.7611088752746582\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 63.8, Loss: 0.8280093669891357\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 61.9, Loss: 0.8584706783294678\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 61.1, Loss: 0.8682000637054443\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 61.8, Loss: 0.7964387536048889\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 61.0, Loss: 0.7274913787841797\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 62.0, Loss: 0.780388593673706\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 61.5, Loss: 0.7087600827217102\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 62.8, Loss: 0.6662200689315796\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 63.1, Loss: 0.8199085593223572\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 62.4, Loss: 1.0687079429626465\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 62.7, Loss: 0.7727506756782532\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 62.1, Loss: 0.8500891923904419\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 62.1, Loss: 0.9698374271392822\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 64.0, Loss: 0.8998501300811768\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-359-f506ce6366cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims-ml/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-327-7abe1162f937>\u001b[0m in \u001b[0;36mSNLI_collate_func\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m         padded_vec1 = np.pad(np.array(datum[0]), \n\u001b[1;32m     21\u001b[0m                                 \u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_SENTENCE_LENGTH\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdatum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                 mode=\"constant\", constant_values=0)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0msent1_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_vec1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list of np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         padded_vec2 = np.pad(np.array(datum[1]), \n",
      "\u001b[0;32m~/anaconda3/envs/aims-ml/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'end_values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                 kwargs[i] = _normalize_shape(narray, kwargs[i],\n\u001b[0;32m-> 1239\u001b[0;31m                                              cast_to_int=False)\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m# Drop back to old, slower np.apply_along_axis mode for user-supplied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims-ml/lib/python3.6/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_normalize_shape\u001b[0;34m(ndarray, shape, cast_to_int)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mshape_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Unable to create correctly shaped tuple from %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims-ml/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_to\u001b[0;34m(array, shape, subok)\u001b[0m\n\u001b[1;32m    174\u001b[0m            [1, 2, 3]])\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aims-ml/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_to\u001b[0;34m(array, shape, subok, readonly)\u001b[0m\n\u001b[1;32m    126\u001b[0m     it = np.nditer(\n\u001b[1;32m    127\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi_index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'refs_ok'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zerosize_ok'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         op_flags=[op_flag], itershape=shape, order='C')\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# never really has writebackifcopy semantics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sent1,sent2,length1,length2, labels in loader:\n",
    "        outputs = F.softmax(model(sent1,sent2,length1,length2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sent1,sent2,length1,length2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sent1, sent2, length1, length2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step() #step() method: updates the parameters,can be called once the gradients are computed using e.g. backward().\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Loss: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "- Varying the size of the vocabulary and embedding dimensions\n",
    "- Experiment with different ways of interacting the two encoded sentences\n",
    "(concatenation, element-wise multiplication, etc) (This is required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size_list = [8000,10000,12000]\n",
    "token2id, id2token = build_vocab(all_train_tokens,max_vocab_size)\n",
    "snli_train_sent1_indices = token2index_dataset(snli_train['sentence1'])\n",
    "snli_train_sent2_indices = token2index_dataset(snli_train['sentence2'])\n",
    "snli_val_sent1_indices = token2index_dataset(snli_val['sentence1'])\n",
    "snli_val_sent2_indices = token2index_dataset(snli_val['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1\tsentence2\tlabel\tgenre\r\n"
     ]
    }
   ],
   "source": [
    "!head -1 'data/mnli_train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   20001 data/mnli_train.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l \"data/mnli_train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['telephone', 'fiction', 'slate', 'government', 'travel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train.genre.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "telephone     4270\n",
       "slate         4026\n",
       "travel        3985\n",
       "government    3883\n",
       "fiction       3836\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "government    1016\n",
       "telephone     1005\n",
       "slate         1002\n",
       "fiction        995\n",
       "travel         982\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val.genre.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "#tokenize train/val datasets\n",
    "#val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "mnli_val1 = tokenize_dataset(mnli_val,istrain=False)\n",
    "mnli_val1.to_pickle(\"data/mnli_val.p\")\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "mnli_train1, all_train_tokens = tokenize_dataset(mnli_train,istrain=True)\n",
    "mnli_train1.to_pickle(\"data/mnli_train.p\")\n",
    "pkl.dump(all_train_tokens, open(\"data/all_train_tokens_mnli.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[and, now, that, was, in, fifty, one, that, 's...</td>\n",
       "      <td>[it, was, already, a, problem, forty, years, a...</td>\n",
       "      <td>2</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[jon, could, smell, baked, bread, on, the, air...</td>\n",
       "      <td>[jon, smelt, food, in, the, air, and, was, hun...</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[it, will, be, like, italian, basketball, with...</td>\n",
       "      <td>[this, type, of, italian, basketball, is, noth...</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[well, i, think, that, 's, about, uh, that, 's...</td>\n",
       "      <td>[sorry, but, we, are, not, done, just, yet]</td>\n",
       "      <td>0</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[good, job, tenure, that, is, --, because, in,...</td>\n",
       "      <td>[dr., quinn, medicine, woman, was, worked, on,...</td>\n",
       "      <td>1</td>\n",
       "      <td>slate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0  [and, now, that, was, in, fifty, one, that, 's...   \n",
       "1  [jon, could, smell, baked, bread, on, the, air...   \n",
       "2  [it, will, be, like, italian, basketball, with...   \n",
       "3  [well, i, think, that, 's, about, uh, that, 's...   \n",
       "4  [good, job, tenure, that, is, --, because, in,...   \n",
       "\n",
       "                                           sentence2  label      genre  \n",
       "0  [it, was, already, a, problem, forty, years, a...      2  telephone  \n",
       "1  [jon, smelt, food, in, the, air, and, was, hun...      2    fiction  \n",
       "2  [this, type, of, italian, basketball, is, noth...      0  telephone  \n",
       "3        [sorry, but, we, are, not, done, just, yet]      0  telephone  \n",
       "4  [dr., quinn, medicine, woman, was, worked, on,...      1      slate  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_val = pkl.load(open(\"data/mnli_val.p\", \"rb\"))\n",
    "mnli_train = pkl.load(open(\"data/mnli_train.p\", \"rb\"))\n",
    "mnli_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[jon, could, smell, baked, bread, on, the, air...</td>\n",
       "      <td>[jon, smelt, food, in, the, air, and, was, hun...</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[jon, turned, to, adrin, vrenna, and, san'doro]</td>\n",
       "      <td>[jon, walked, away, without, acknowledging, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[slowly, tommy, spoke]</td>\n",
       "      <td>[tommy, did, not, talk, quickly]</td>\n",
       "      <td>1</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[he, far, underestimated, the, boy]</td>\n",
       "      <td>[he, underestimated, how, good, of, a, cook, t...</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[still, connected, up, to, the, polygraph, mac...</td>\n",
       "      <td>[i, did, n't, care, what, the, polygraph, showed]</td>\n",
       "      <td>0</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3831</td>\n",
       "      <td>[sharp, spikes, lined, the, pit, aiming, both,...</td>\n",
       "      <td>[the, pit, was, lined, with, smooth, dirt]</td>\n",
       "      <td>0</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3832</td>\n",
       "      <td>[he, motioned, to, tommy, to, sit, down, oppos...</td>\n",
       "      <td>[he, told, tommy, to, remain, standing]</td>\n",
       "      <td>0</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3833</td>\n",
       "      <td>[he, fell, back, his, leg, twisting, at, a, wr...</td>\n",
       "      <td>[he, was, not, being, careful]</td>\n",
       "      <td>2</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3834</td>\n",
       "      <td>[he, was, good, better, than, jon, let, on, sa...</td>\n",
       "      <td>[the, kal, said, that, he, was, better, than, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3835</td>\n",
       "      <td>[jon, looked, at, ca'daan, standing, in, front...</td>\n",
       "      <td>[jon, and, ca'daan, are, sitting]</td>\n",
       "      <td>0</td>\n",
       "      <td>fiction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence1  \\\n",
       "0     [jon, could, smell, baked, bread, on, the, air...   \n",
       "1       [jon, turned, to, adrin, vrenna, and, san'doro]   \n",
       "2                                [slowly, tommy, spoke]   \n",
       "3                   [he, far, underestimated, the, boy]   \n",
       "4     [still, connected, up, to, the, polygraph, mac...   \n",
       "...                                                 ...   \n",
       "3831  [sharp, spikes, lined, the, pit, aiming, both,...   \n",
       "3832  [he, motioned, to, tommy, to, sit, down, oppos...   \n",
       "3833  [he, fell, back, his, leg, twisting, at, a, wr...   \n",
       "3834  [he, was, good, better, than, jon, let, on, sa...   \n",
       "3835  [jon, looked, at, ca'daan, standing, in, front...   \n",
       "\n",
       "                                              sentence2  label    genre  \n",
       "0     [jon, smelt, food, in, the, air, and, was, hun...      2  fiction  \n",
       "1     [jon, walked, away, without, acknowledging, th...      0  fiction  \n",
       "2                      [tommy, did, not, talk, quickly]      1  fiction  \n",
       "3     [he, underestimated, how, good, of, a, cook, t...      2  fiction  \n",
       "4     [i, did, n't, care, what, the, polygraph, showed]      0  fiction  \n",
       "...                                                 ...    ...      ...  \n",
       "3831         [the, pit, was, lined, with, smooth, dirt]      0  fiction  \n",
       "3832            [he, told, tommy, to, remain, standing]      0  fiction  \n",
       "3833                     [he, was, not, being, careful]      2  fiction  \n",
       "3834  [the, kal, said, that, he, was, better, than, ...      1  fiction  \n",
       "3835                  [jon, and, ca'daan, are, sitting]      0  fiction  \n",
       "\n",
       "[3836 rows x 4 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_train[mnli_train['genre']=='fiction'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mnli_train.genre.unique():\n",
    "    mnli_train[mnli_train['genre']==i].reset_index(drop=True).to_pickle('data/MNLI/mnli_train_'+i+'.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mnli_val.genre.unique():\n",
    "    mnli_val[mnli_val['genre']==i].reset_index(drop=True).to_pickle('data/MNLI/mnli_val_'+i+'.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  load mnli tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_train_telephone = pkl.load(open(\"data/MNLI/mnli_train_telephone.p\", \"rb\"))\n",
    "mnli_train_fiction = pkl.load(open(\"data/MNLI/mnli_train_fiction.p\", \"rb\"))\n",
    "mnli_train_slate = pkl.load(open(\"data/MNLI/mnli_train_slate.p\", \"rb\"))\n",
    "mnli_train_government = pkl.load(open(\"data/MNLI/mnli_train_government.p\", \"rb\"))\n",
    "mnli_train_travel = pkl.load(open(\"data/MNLI/mnli_train_travel.p\", \"rb\"))\n",
    "mnli_val_telephone = pkl.load(open(\"data/MNLI/mnli_val_telephone.p\", \"rb\"))\n",
    "mnli_val_fiction = pkl.load(open(\"data/MNLI/mnli_val_fiction.p\", \"rb\"))\n",
    "mnli_val_slate = pkl.load(open(\"data/MNLI/mnli_val_slate.p\", \"rb\"))\n",
    "mnli_val_government = pkl.load(open(\"data/MNLI/mnli_val_government.p\", \"rb\"))\n",
    "mnli_val_travel = pkl.load(open(\"data/MNLI/mnli_val_travel.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokens to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "mnli_train_telephone_sent1_indices = token2index_dataset(mnli_train_telephone['sentence1'])\n",
    "mnli_train_telephone_sent2_indices = token2index_dataset(mnli_train_telephone['sentence2'])\n",
    "mnli_val_telephone_sent1_indices = token2index_dataset(mnli_val_telephone['sentence1'])\n",
    "mnli_val_telephone_sent2_indices = token2index_dataset(mnli_val_telephone['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hop', 'skip', 'and', 'a', 'jump', 'there', 'you', 'go', 'well', 'that', \"'s\", 'good', 'well', 'are', 'you', 'uh', 'do', 'you', 'prefer', 'the', 'kind', 'of', 'weather', 'that', 'you', \"'re\", 'getting', 'in', 'dallas', 'over', 'your', 'years', 'in', 'new', 'hampshire', 'or', 'do', 'you', 'miss', 'the', 'winters', 'or']\n",
      "[1, 1, 5, 6, 4040, 29, 13, 103, 64, 9, 12, 82, 64, 16, 13, 33, 25, 13, 1419, 2, 186, 3, 593, 9, 13, 77, 326, 7, 1018, 104, 84, 116, 7, 88, 5539, 37, 25, 13, 747, 2, 6344, 37]\n",
      "['why', 'have', 'you', 'never', 'been', 'to', 'new', 'hampshire']\n",
      "[242, 22, 13, 108, 61, 4, 88, 5539]\n"
     ]
    }
   ],
   "source": [
    "# visualize a random tokenized training example\n",
    "rand_idx = random.randint(0, len(mnli_train_telephone) - 1)\n",
    "print(mnli_train_telephone['sentence1'][rand_idx])\n",
    "print(mnli_train_telephone_sent1_indices[rand_idx])\n",
    "print(mnli_train_telephone['sentence2'][rand_idx])\n",
    "print(mnli_train_telephone_sent2_indices[rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = max(max(mnli_train_telephone['sentence1'].apply(len)),max(mnli_train_telephone['sentence2'].apply(len)))\n",
    "MAX_SENTENCE_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX_SENTENCE_LENGTH = 200\n",
    "class MNLIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sent1_list, sent2_list, label_list):\n",
    "\n",
    "        self.sent1_list = sent1_list\n",
    "        self.sent2_list = sent2_list\n",
    "        self.label_list = label_list\n",
    "        assert (len(self.sent1_list) == len(self.label_list))\n",
    "        assert (len(self.sent2_list) == len(self.label_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx1 = self.sent1_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        token_idx2 = self.sent2_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx1, token_idx2, len(token_idx1), len(token_idx2), label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_telephone_train_dataset = MNLIDataset(mnli_train_telephone_sent1_indices, mnli_train_telephone_sent2_indices,mnli_train_telephone['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNLI_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length: tell dataloader how to pad our sentences\n",
    "    \"\"\"\n",
    "    sent1_list = []\n",
    "    sent2_list = []\n",
    "    label_list = []\n",
    "    length1_list = []\n",
    "    length2_list = []\n",
    "\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length1_list.append(datum[2])\n",
    "        length2_list.append(datum[3])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sent1_list.append(padded_vec1) # list of np.array\n",
    "        padded_vec2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        sent2_list.append(padded_vec2)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(sent1_list)),torch.from_numpy(np.array(sent2_list)),torch.LongTensor(length1_list), torch.LongTensor(length2_list),torch.LongTensor(label_list)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "mnli_telephone_train_loader = torch.utils.data.DataLoader(dataset=mnli_telephone_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)\n",
    "mnli_telephone_val_dataset = SNLIDataset(snli_val_sent1_indices, snli_val_sent2_indices, snli_val['label'].tolist())\n",
    "mnli_telephone_val_loader = torch.utils.data.DataLoader(dataset=mnli_telephone_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=MNLI_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 242])\n",
      "torch.Size([32, 242])\n",
      "torch.Size([32, 484])\n",
      "tensor([16, 54,  8, 36, 27, 22, 28, 28, 22, 27, 11,  9, 13, 44, 94, 47, 16, 31,\n",
      "        19, 37, 32, 52, 39, 30, 26, 49, 35, 27, 14,  7,  9, 50])\n",
      "tensor([13,  6,  3, 27, 11,  5,  6, 14, 15, 13, 11,  6, 12, 14,  6, 11,  7, 18,\n",
      "         8, 11, 13,  7, 15,  8,  9,  5, 12, 13, 10,  3, 13,  5])\n",
      "tensor([0, 1, 0, 1, 2, 2, 2, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 0, 0, 1, 1, 0,\n",
      "        0, 2, 2, 2, 0, 0, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "### checking the first batch of data loader\n",
    "for i, (sent1,sent2,length1,length2, labels) in enumerate(mnli_telephone_train_loader):\n",
    "#     print(sent1)\n",
    "    print(sent1.shape)\n",
    "#     print(sent2)\n",
    "    print(sent2.shape)\n",
    "    print((torch.cat((sent1, sent2), 1)).shape)\n",
    "    print(length1)\n",
    "    print(length2)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Finetuneing on MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocabulary_size': 7000,\n",
       " 'embedding_dimension': 30,\n",
       " 'hidden_dimension1': 30,\n",
       " 'hidden_dimension2': 30,\n",
       " 'sentences_interaction': 'concatenate',\n",
       " 'optimizer': 'Adam',\n",
       " 'learning_rate': 0.01,\n",
       " 'lr_desc_per_ep': 0.0009,\n",
       " 'weight_decay': 1e-06,\n",
       " 'drop_out': 0.05,\n",
       " 'max_sentence_length': 50,\n",
       " 'best_val_acc': 68.7,\n",
       " '@epoch': 8,\n",
       " 'num_epochs': 10,\n",
       " '@step': 101,\n",
       " 'total_steps': 3125,\n",
       " '@train_loss': 0.483,\n",
       " 'time_per_epoch(min)': 5.336699211597443}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best NN model\n",
    "best = pkl.load(open(\"snli_best_model.pkl\", \"rb\"))\n",
    "best['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7002, 30])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = best['model']\n",
    "best_model.embed.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [101/134], Validation Acc: 37.6, Loss: 1.8245795965194702\n",
      "Epoch: [2/10], Step: [101/134], Validation Acc: 38.0, Loss: 1.348197340965271\n",
      "Epoch: [3/10], Step: [101/134], Validation Acc: 37.4, Loss: 1.1927039623260498\n",
      "Epoch: [4/10], Step: [101/134], Validation Acc: 37.7, Loss: 1.2358551025390625\n",
      "Epoch: [5/10], Step: [101/134], Validation Acc: 37.1, Loss: 1.0580377578735352\n",
      "Epoch: [6/10], Step: [101/134], Validation Acc: 36.4, Loss: 1.0513455867767334\n",
      "Epoch: [7/10], Step: [101/134], Validation Acc: 36.7, Loss: 1.0686239004135132\n",
      "Epoch: [8/10], Step: [101/134], Validation Acc: 37.0, Loss: 1.0709761381149292\n",
      "Epoch: [9/10], Step: [101/134], Validation Acc: 36.4, Loss: 1.111294150352478\n",
      "Epoch: [10/10], Step: [101/134], Validation Acc: 36.9, Loss: 0.9979006052017212\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sent1,sent2,length1,length2, labels in loader:\n",
    "        outputs = F.softmax(model(sent1,length1,sent2,length2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sent1,sent2,length1,length2, labels) in enumerate(mnli_telephone_train_loader):\n",
    "        best_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(sent1, length1, sent2, length2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(mnli_telephone_val_loader, best_model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Loss: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(mnli_telephone_train_loader), val_acc, loss))\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=0.01, weight_decay=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [101/3125], Validation Acc: 35.1, Loss: 1.116905689239502\n",
      "Epoch: [1/5], Step: [201/3125], Validation Acc: 33.6, Loss: 1.1030097007751465\n",
      "Epoch: [1/5], Step: [301/3125], Validation Acc: 35.2, Loss: 1.114428997039795\n",
      "Epoch: [1/5], Step: [401/3125], Validation Acc: 36.1, Loss: 1.1081569194793701\n",
      "Epoch: [1/5], Step: [501/3125], Validation Acc: 38.9, Loss: 1.0869009494781494\n",
      "Epoch: [1/5], Step: [601/3125], Validation Acc: 38.5, Loss: 1.105239987373352\n",
      "Epoch: [1/5], Step: [701/3125], Validation Acc: 38.5, Loss: 1.0369211435317993\n",
      "Epoch: [1/5], Step: [801/3125], Validation Acc: 44.0, Loss: 1.0427151918411255\n",
      "Epoch: [1/5], Step: [901/3125], Validation Acc: 44.6, Loss: 1.0304932594299316\n",
      "Epoch: [1/5], Step: [1001/3125], Validation Acc: 49.8, Loss: 1.1065162420272827\n",
      "Epoch: [1/5], Step: [1101/3125], Validation Acc: 54.8, Loss: 1.0114941596984863\n",
      "Epoch: [1/5], Step: [1201/3125], Validation Acc: 56.0, Loss: 0.9452487230300903\n",
      "Epoch: [1/5], Step: [1301/3125], Validation Acc: 54.5, Loss: 0.8917548060417175\n",
      "Epoch: [1/5], Step: [1401/3125], Validation Acc: 54.2, Loss: 0.9469119906425476\n",
      "Epoch: [1/5], Step: [1501/3125], Validation Acc: 58.3, Loss: 0.9226313829421997\n",
      "Epoch: [1/5], Step: [1601/3125], Validation Acc: 58.6, Loss: 1.0154865980148315\n",
      "Epoch: [1/5], Step: [1701/3125], Validation Acc: 57.5, Loss: 1.0415396690368652\n",
      "Epoch: [1/5], Step: [1801/3125], Validation Acc: 58.3, Loss: 0.9287747144699097\n",
      "Epoch: [1/5], Step: [1901/3125], Validation Acc: 57.7, Loss: 0.890082061290741\n",
      "Epoch: [1/5], Step: [2001/3125], Validation Acc: 59.0, Loss: 0.9469321966171265\n",
      "Epoch: [1/5], Step: [2101/3125], Validation Acc: 58.2, Loss: 0.8530976176261902\n",
      "Epoch: [1/5], Step: [2201/3125], Validation Acc: 58.2, Loss: 0.8524788022041321\n",
      "Epoch: [1/5], Step: [2301/3125], Validation Acc: 56.3, Loss: 0.8405203819274902\n",
      "Epoch: [1/5], Step: [2401/3125], Validation Acc: 57.6, Loss: 0.9353123307228088\n",
      "Epoch: [1/5], Step: [2501/3125], Validation Acc: 57.5, Loss: 0.8829628229141235\n",
      "Epoch: [1/5], Step: [2601/3125], Validation Acc: 60.1, Loss: 0.9745637774467468\n",
      "Epoch: [1/5], Step: [2701/3125], Validation Acc: 58.9, Loss: 0.8867486715316772\n",
      "Epoch: [1/5], Step: [2801/3125], Validation Acc: 60.1, Loss: 1.0979506969451904\n",
      "Epoch: [1/5], Step: [2901/3125], Validation Acc: 59.7, Loss: 0.8488107919692993\n",
      "Epoch: [1/5], Step: [3001/3125], Validation Acc: 58.2, Loss: 0.9381132125854492\n",
      "Epoch: [1/5], Step: [3101/3125], Validation Acc: 59.4, Loss: 1.1466583013534546\n",
      "Epoch: [2/5], Step: [101/3125], Validation Acc: 59.7, Loss: 1.0970913171768188\n",
      "Epoch: [2/5], Step: [201/3125], Validation Acc: 58.9, Loss: 0.726529598236084\n",
      "Epoch: [2/5], Step: [301/3125], Validation Acc: 59.0, Loss: 0.6676735281944275\n",
      "Epoch: [2/5], Step: [401/3125], Validation Acc: 61.2, Loss: 0.7740294933319092\n",
      "Epoch: [2/5], Step: [501/3125], Validation Acc: 59.8, Loss: 0.7100878953933716\n",
      "Epoch: [2/5], Step: [601/3125], Validation Acc: 61.0, Loss: 0.815282940864563\n",
      "Epoch: [2/5], Step: [701/3125], Validation Acc: 61.0, Loss: 0.9867972731590271\n",
      "Epoch: [2/5], Step: [801/3125], Validation Acc: 60.0, Loss: 0.8424674868583679\n",
      "Epoch: [2/5], Step: [901/3125], Validation Acc: 61.2, Loss: 0.9002037048339844\n",
      "Epoch: [2/5], Step: [1001/3125], Validation Acc: 59.7, Loss: 0.8776283860206604\n",
      "Epoch: [2/5], Step: [1101/3125], Validation Acc: 59.7, Loss: 0.8442652225494385\n",
      "Epoch: [2/5], Step: [1201/3125], Validation Acc: 60.9, Loss: 0.9654255509376526\n",
      "Epoch: [2/5], Step: [1301/3125], Validation Acc: 61.5, Loss: 0.7426732778549194\n",
      "Epoch: [2/5], Step: [1401/3125], Validation Acc: 60.6, Loss: 0.7724592685699463\n",
      "Epoch: [2/5], Step: [1501/3125], Validation Acc: 61.2, Loss: 0.968264102935791\n",
      "Epoch: [2/5], Step: [1601/3125], Validation Acc: 60.8, Loss: 0.9075129628181458\n",
      "Epoch: [2/5], Step: [1701/3125], Validation Acc: 63.1, Loss: 0.7679418325424194\n",
      "Epoch: [2/5], Step: [1801/3125], Validation Acc: 62.8, Loss: 0.7420844435691833\n",
      "Epoch: [2/5], Step: [1901/3125], Validation Acc: 62.8, Loss: 0.9968690276145935\n",
      "Epoch: [2/5], Step: [2001/3125], Validation Acc: 61.9, Loss: 0.7593984603881836\n",
      "Epoch: [2/5], Step: [2101/3125], Validation Acc: 61.8, Loss: 1.0662872791290283\n",
      "Epoch: [2/5], Step: [2201/3125], Validation Acc: 60.9, Loss: 0.8181093335151672\n",
      "Epoch: [2/5], Step: [2301/3125], Validation Acc: 60.9, Loss: 0.6464132070541382\n",
      "Epoch: [2/5], Step: [2401/3125], Validation Acc: 60.0, Loss: 0.9623790979385376\n",
      "Epoch: [2/5], Step: [2501/3125], Validation Acc: 61.8, Loss: 0.8234599232673645\n",
      "Epoch: [2/5], Step: [2601/3125], Validation Acc: 60.8, Loss: 0.6332172751426697\n",
      "Epoch: [2/5], Step: [2701/3125], Validation Acc: 61.9, Loss: 0.8494910001754761\n",
      "Epoch: [2/5], Step: [2801/3125], Validation Acc: 62.0, Loss: 0.788379430770874\n",
      "Epoch: [2/5], Step: [2901/3125], Validation Acc: 64.5, Loss: 0.9158216118812561\n",
      "Epoch: [2/5], Step: [3001/3125], Validation Acc: 60.3, Loss: 0.9392775297164917\n",
      "Epoch: [2/5], Step: [3101/3125], Validation Acc: 61.6, Loss: 0.9088725447654724\n",
      "Epoch: [3/5], Step: [101/3125], Validation Acc: 62.3, Loss: 0.8383700251579285\n",
      "Epoch: [3/5], Step: [201/3125], Validation Acc: 62.3, Loss: 0.7851022481918335\n",
      "Epoch: [3/5], Step: [301/3125], Validation Acc: 63.4, Loss: 0.9923015236854553\n",
      "Epoch: [3/5], Step: [401/3125], Validation Acc: 62.7, Loss: 0.6839410662651062\n",
      "Epoch: [3/5], Step: [501/3125], Validation Acc: 62.3, Loss: 0.8714919686317444\n",
      "Epoch: [3/5], Step: [601/3125], Validation Acc: 62.7, Loss: 0.9011373519897461\n",
      "Epoch: [3/5], Step: [701/3125], Validation Acc: 62.1, Loss: 0.7790895104408264\n",
      "Epoch: [3/5], Step: [801/3125], Validation Acc: 62.2, Loss: 1.0071121454238892\n",
      "Epoch: [3/5], Step: [901/3125], Validation Acc: 63.1, Loss: 0.6932833790779114\n",
      "Epoch: [3/5], Step: [1001/3125], Validation Acc: 63.0, Loss: 0.870337188243866\n",
      "Epoch: [3/5], Step: [1101/3125], Validation Acc: 62.7, Loss: 0.7710242867469788\n",
      "Epoch: [3/5], Step: [1201/3125], Validation Acc: 62.3, Loss: 0.8929983377456665\n",
      "Epoch: [3/5], Step: [1301/3125], Validation Acc: 62.8, Loss: 0.707929253578186\n",
      "Epoch: [3/5], Step: [1401/3125], Validation Acc: 62.1, Loss: 0.8353199362754822\n",
      "Epoch: [3/5], Step: [1501/3125], Validation Acc: 61.0, Loss: 0.7991558909416199\n",
      "Epoch: [3/5], Step: [1601/3125], Validation Acc: 62.4, Loss: 0.638709545135498\n",
      "Epoch: [3/5], Step: [1701/3125], Validation Acc: 62.0, Loss: 0.8970715999603271\n",
      "Epoch: [3/5], Step: [1801/3125], Validation Acc: 62.8, Loss: 1.1403565406799316\n",
      "Epoch: [3/5], Step: [1901/3125], Validation Acc: 60.6, Loss: 0.6288174390792847\n",
      "Epoch: [3/5], Step: [2001/3125], Validation Acc: 62.4, Loss: 0.7093608379364014\n",
      "Epoch: [3/5], Step: [2101/3125], Validation Acc: 62.3, Loss: 0.85770583152771\n",
      "Epoch: [3/5], Step: [2201/3125], Validation Acc: 61.2, Loss: 0.8020843863487244\n",
      "Epoch: [3/5], Step: [2301/3125], Validation Acc: 62.4, Loss: 0.7678335905075073\n",
      "Epoch: [3/5], Step: [2401/3125], Validation Acc: 63.0, Loss: 0.699383020401001\n",
      "Epoch: [3/5], Step: [2501/3125], Validation Acc: 61.9, Loss: 0.7087824940681458\n",
      "Epoch: [3/5], Step: [2601/3125], Validation Acc: 63.1, Loss: 0.8537898063659668\n",
      "Epoch: [3/5], Step: [2701/3125], Validation Acc: 63.5, Loss: 0.7872914671897888\n",
      "Epoch: [3/5], Step: [2801/3125], Validation Acc: 62.6, Loss: 0.6714825630187988\n",
      "Epoch: [3/5], Step: [2901/3125], Validation Acc: 61.1, Loss: 0.8583195209503174\n",
      "Epoch: [3/5], Step: [3001/3125], Validation Acc: 62.7, Loss: 0.851641058921814\n",
      "Epoch: [3/5], Step: [3101/3125], Validation Acc: 61.5, Loss: 0.7630119323730469\n",
      "Epoch: [4/5], Step: [101/3125], Validation Acc: 63.3, Loss: 0.7620267271995544\n",
      "Epoch: [4/5], Step: [201/3125], Validation Acc: 61.9, Loss: 0.7803070545196533\n",
      "Epoch: [4/5], Step: [301/3125], Validation Acc: 62.9, Loss: 0.9455287456512451\n",
      "Epoch: [4/5], Step: [401/3125], Validation Acc: 61.9, Loss: 0.7300349473953247\n",
      "Epoch: [4/5], Step: [501/3125], Validation Acc: 62.1, Loss: 0.6351402997970581\n",
      "Epoch: [4/5], Step: [601/3125], Validation Acc: 62.2, Loss: 0.7220221161842346\n",
      "Epoch: [4/5], Step: [701/3125], Validation Acc: 61.3, Loss: 0.8301058411598206\n",
      "Epoch: [4/5], Step: [801/3125], Validation Acc: 62.9, Loss: 0.8043452501296997\n",
      "Epoch: [4/5], Step: [901/3125], Validation Acc: 62.7, Loss: 0.8119043111801147\n",
      "Epoch: [4/5], Step: [1001/3125], Validation Acc: 63.7, Loss: 0.8919413685798645\n",
      "Epoch: [4/5], Step: [1101/3125], Validation Acc: 63.0, Loss: 0.7049845457077026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4/5], Step: [1201/3125], Validation Acc: 64.5, Loss: 0.5564201474189758\n",
      "Epoch: [4/5], Step: [1301/3125], Validation Acc: 62.9, Loss: 0.6677322387695312\n",
      "Epoch: [4/5], Step: [1401/3125], Validation Acc: 63.2, Loss: 0.7339513897895813\n",
      "Epoch: [4/5], Step: [1501/3125], Validation Acc: 62.0, Loss: 0.8194581866264343\n",
      "Epoch: [4/5], Step: [1601/3125], Validation Acc: 62.3, Loss: 0.7582756280899048\n",
      "Epoch: [4/5], Step: [1701/3125], Validation Acc: 61.7, Loss: 0.6723196506500244\n",
      "Epoch: [4/5], Step: [1801/3125], Validation Acc: 63.2, Loss: 0.6262174248695374\n",
      "Epoch: [4/5], Step: [1901/3125], Validation Acc: 63.8, Loss: 0.6238638162612915\n",
      "Epoch: [4/5], Step: [2001/3125], Validation Acc: 63.9, Loss: 0.6553499102592468\n",
      "Epoch: [4/5], Step: [2101/3125], Validation Acc: 62.9, Loss: 0.7499513030052185\n",
      "Epoch: [4/5], Step: [2201/3125], Validation Acc: 64.4, Loss: 0.7213110327720642\n",
      "Epoch: [4/5], Step: [2301/3125], Validation Acc: 64.2, Loss: 0.7678834795951843\n",
      "Epoch: [4/5], Step: [2401/3125], Validation Acc: 63.9, Loss: 0.7190318703651428\n",
      "Epoch: [4/5], Step: [2501/3125], Validation Acc: 63.4, Loss: 0.8759229779243469\n",
      "Epoch: [4/5], Step: [2601/3125], Validation Acc: 63.7, Loss: 0.6147217750549316\n",
      "Epoch: [4/5], Step: [2701/3125], Validation Acc: 63.3, Loss: 0.8583499789237976\n",
      "Epoch: [4/5], Step: [2801/3125], Validation Acc: 61.9, Loss: 0.8321362733840942\n",
      "Epoch: [4/5], Step: [2901/3125], Validation Acc: 62.8, Loss: 0.7424664497375488\n",
      "Epoch: [4/5], Step: [3001/3125], Validation Acc: 63.2, Loss: 0.8332908153533936\n",
      "Epoch: [4/5], Step: [3101/3125], Validation Acc: 63.7, Loss: 0.6183973550796509\n",
      "Epoch: [5/5], Step: [101/3125], Validation Acc: 63.7, Loss: 0.8477010726928711\n",
      "Epoch: [5/5], Step: [201/3125], Validation Acc: 64.6, Loss: 0.5530855059623718\n",
      "Epoch: [5/5], Step: [301/3125], Validation Acc: 63.1, Loss: 0.5553411841392517\n",
      "Epoch: [5/5], Step: [401/3125], Validation Acc: 63.8, Loss: 0.7212428450584412\n",
      "Epoch: [5/5], Step: [501/3125], Validation Acc: 64.5, Loss: 0.8080652952194214\n",
      "Epoch: [5/5], Step: [601/3125], Validation Acc: 64.3, Loss: 0.7502273321151733\n",
      "Epoch: [5/5], Step: [701/3125], Validation Acc: 63.6, Loss: 1.0615882873535156\n",
      "Epoch: [5/5], Step: [801/3125], Validation Acc: 62.9, Loss: 0.6144611835479736\n",
      "Epoch: [5/5], Step: [901/3125], Validation Acc: 63.6, Loss: 0.9943614602088928\n",
      "Epoch: [5/5], Step: [1001/3125], Validation Acc: 62.3, Loss: 0.7695281505584717\n",
      "Epoch: [5/5], Step: [1101/3125], Validation Acc: 62.8, Loss: 0.6364924907684326\n",
      "Epoch: [5/5], Step: [1201/3125], Validation Acc: 64.4, Loss: 0.7304889559745789\n",
      "Epoch: [5/5], Step: [1301/3125], Validation Acc: 63.5, Loss: 1.0106796026229858\n",
      "Epoch: [5/5], Step: [1401/3125], Validation Acc: 65.7, Loss: 0.8685046434402466\n",
      "Epoch: [5/5], Step: [1501/3125], Validation Acc: 64.4, Loss: 0.6315056085586548\n",
      "Epoch: [5/5], Step: [1601/3125], Validation Acc: 63.9, Loss: 0.6043731570243835\n",
      "Epoch: [5/5], Step: [1701/3125], Validation Acc: 64.7, Loss: 0.6167111396789551\n",
      "Epoch: [5/5], Step: [1801/3125], Validation Acc: 63.8, Loss: 0.6956670880317688\n",
      "Epoch: [5/5], Step: [1901/3125], Validation Acc: 63.7, Loss: 0.7904825806617737\n",
      "Epoch: [5/5], Step: [2001/3125], Validation Acc: 64.4, Loss: 0.5882167816162109\n",
      "Epoch: [5/5], Step: [2101/3125], Validation Acc: 62.7, Loss: 0.8501295447349548\n",
      "Epoch: [5/5], Step: [2201/3125], Validation Acc: 63.3, Loss: 0.7015377283096313\n",
      "Epoch: [5/5], Step: [2301/3125], Validation Acc: 63.3, Loss: 0.8060303330421448\n",
      "Epoch: [5/5], Step: [2401/3125], Validation Acc: 64.6, Loss: 0.5908716917037964\n",
      "Epoch: [5/5], Step: [2501/3125], Validation Acc: 64.4, Loss: 0.7785186171531677\n",
      "Epoch: [5/5], Step: [2601/3125], Validation Acc: 65.9, Loss: 0.5361767411231995\n",
      "Epoch: [5/5], Step: [2701/3125], Validation Acc: 64.6, Loss: 0.9641358852386475\n",
      "Epoch: [5/5], Step: [2801/3125], Validation Acc: 63.6, Loss: 0.8026245832443237\n",
      "Epoch: [5/5], Step: [2901/3125], Validation Acc: 64.8, Loss: 0.9144676327705383\n",
      "Epoch: [5/5], Step: [3001/3125], Validation Acc: 63.0, Loss: 0.920575737953186\n",
      "Epoch: [5/5], Step: [3101/3125], Validation Acc: 64.2, Loss: 0.7147355079650879\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "python snli_save_best_model.py --model-file ./snil_best_model.pkl --h1-dim 30 --h2-dim 30 --max-vocab-size 7000 --ebd-dim 30 --sent-interaction concatenate --optimizer Adam --lr 0.01 --lr-desc-per-ep 0.0009 --epochs 10 --w-dec 0.000001 --dropout 0.05 --batch-size 32\"\"\"\n",
    "num_epochs = 5 # number epoch to train\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for sent1,sent2,length1,length2, labels in loader:\n",
    "        outputs = F.softmax(model(sent1,length1,sent2,length2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (sent1,sent2,length1,length2, labels) in enumerate(train_loader):\n",
    "        best_model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_model(sent1, length1, sent2, length2)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, best_model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}, Loss: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['999994', '300']\n",
      "301\n",
      "[',', '0.1073', '0.0089', '0.0006', '0.0055', '-0.0646', '-0.0600', '0.0450', '-0.0133', '-0.0357', '0.0430', '-0.0356', '-0.0032', '0.0073', '-0.0001', '0.0258', '-0.0166', '0.0075', '0.0686', '0.0392', '0.0753', '0.0115', '-0.0087', '0.0421', '0.0265', '-0.0601', '0.2420', '0.0199', '-0.0739', '-0.0031', '-0.0263', '-0.0062', '0.0168', '-0.0357', '-0.0249', '0.0190', '-0.0184', '-0.0537', '0.1420', '0.0600', '0.0226', '-0.0038', '-0.0675', '-0.0036', '-0.0080', '0.0570', '0.0208', '0.0223', '-0.0256', '-0.0153', '0.0022', '-0.0482', '0.0131', '-0.6016', '-0.0088', '0.0106', '0.0229', '0.0336', '0.0071', '0.0887', '0.0237', '-0.0290', '-0.0405', '-0.0125', '0.0147', '0.0475', '0.0647', '0.0474', '0.0199', '0.0408', '0.0322', '0.0036', '0.0350', '-0.0723', '-0.0305', '0.0184', '-0.0026', '0.0240', '-0.0160', '-0.0308', '0.0434', '0.0147', '-0.0457', '-0.0267', '-0.1703', '-0.0099', '0.0417', '0.0235', '-0.0260', '-0.1519', '-0.0116', '-0.0306', '-0.0413', '0.0330', '0.0723', '0.0365', '-0.0001', '0.0042', '0.0346', '0.0277', '-0.0305', '0.0784', '-0.0404', '0.0187', '-0.0225', '-0.0206', '-0.0179', '-0.2428', '0.0669', '0.0523', '0.0527', '0.0149', '-0.0708', '-0.0987', '0.0263', '-0.0611', '0.0302', '0.0216', '0.0313', '-0.0140', '-0.2495', '-0.0346', '-0.0480', '0.0250', '0.2130', '-0.0330', '-0.1553', '-0.0292', '-0.0346', '0.1074', '0.0010', '-0.0117', '-0.0057', '-0.1280', '-0.0038', '0.0130', '-0.1157', '-0.0108', '0.0275', '0.0158', '-0.0169', '0.0070', '0.0247', '0.0510', '1.0292', '-0.0283', '-0.0310', '-0.0026', '-0.0343', '0.0578', '0.0444', '0.0812', '-0.0211', '-0.0872', '0.0169', '0.0499', '0.0485', '0.0227', '-0.0323', '-0.0035', '0.0435', '-0.0275', '0.0154', '0.0135', '-0.0484', '-0.0699', '-0.0502', '0.2745', '-0.0003', '-0.0371', '0.0517', '-0.0908', '0.0013', '0.0360', '0.0280', '0.0839', '0.0980', '-0.0490', '-0.2423', '-0.0142', '0.0024', '-0.0207', '0.0012', '0.0088', '-0.0143', '-0.0197', '0.0515', '-0.0085', '0.0257', '0.2154', '0.0301', '0.0211', '0.0530', '-0.0005', '0.0177', '0.0016', '-0.0053', '-0.0162', '-0.0223', '-0.1862', '0.0398', '0.0658', '-0.0962', '-0.0076', '-0.0075', '-0.0342', '-0.0265', '0.0420', '0.0522', '-0.0266', '0.0201', '-0.1331', '-0.0367', '0.0351', '0.0518', '-0.0087', '0.0599', '-0.1086', '-0.0188', '0.0481', '0.0105', '-0.0060', '0.0151', '-0.0031', '0.0077', '-0.0276', '-0.0373', '-0.0203', '0.0472', '0.0246', '0.1440', '0.0542', '-0.0225', '0.2495', '0.1617', '0.0038', '0.1119', '-0.0230', '-0.0785', '0.0250', '-0.0616', '-0.0485', '0.0225', '0.0281', '0.0041', '0.0112', '0.0172', '0.0291', '-0.0282', '0.0026', '0.4055', '0.0392', '0.0088', '0.0228', '0.0299', '0.1195', '0.0545', '-0.0020', '0.0020', '0.0490', '0.0145', '-0.0086', '0.0098', '-0.0236', '0.0171', '-0.0765', '-0.0400', '0.0128', '0.0011', '0.0042', '0.0244', '0.0075', '0.0200', '0.0201', '0.0196', '-0.0377', '-0.0432', '-0.0073', '-0.0021', '0.0183', '0.0076', '0.1805', '-0.0551', '0.0075', '-0.0516', '0.0420', '-0.0068', '-0.0711', '-0.1408', '0.0504', '0.0276', '0.0470', '0.0323', '-0.0219', '0.0010', '0.0089', '0.0276', '0.0186', '0.0050', '0.1173', '-0.0400']\n"
     ]
    }
   ],
   "source": [
    "with open('data/wiki-news-300d-1M.vec') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i > 1:\n",
    "            break\n",
    "        if i > 0:\n",
    "            print(len(line.split()))\n",
    "        print(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def load_pretrained_ebd(fname,max_vocab_size):\n",
    "    with open('data/wiki-news-300d-1M.vec') as f:\n",
    "        word_ebd = np.zeros((max_vocab_size+2, 300))\n",
    "        token2id = {}\n",
    "        id2token = {}\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            if i ==0:\n",
    "                continue\n",
    "            if i >= max_vocab_size: \n",
    "                break\n",
    "            l = line.split()\n",
    "            word_ebd[i+1, :] = np.asarray(l[1:]) #0,1 reserved for pad and unk\n",
    "            token2id[l[0]] = i+1\n",
    "            id2token[i+1] = l[0]\n",
    "    return word_ebd, token2id, id2token\n",
    "\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "word_ebd, token2id,id2token = load_pretrained_ebd('data/wiki-news-300d-1M.vec',MAX_VOCAB_SIZE)\n",
    "word_ebd[1] = np.random.rand(300,) # wait to be trained\n",
    "token2id['PAD']=0\n",
    "token2id['UNK']=1\n",
    "id2token[0]='PAD'\n",
    "id2token[1]='UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent1\n",
      " tensor([32,  1,  2, 32,  0,  1])\n",
      "pretrained_sent1:\n",
      " tensor([32,  0,  2, 32,  0,  0])\n",
      "out1:fixed_embed\n",
      " tensor([[-0.1202,  0.0700,  0.1030,  ...,  0.3267, -0.0606,  0.0025],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1073,  0.0089,  0.0006,  ...,  0.0050,  0.1173, -0.0400],\n",
      "        [-0.1202,  0.0700,  0.1030,  ...,  0.3267, -0.0606,  0.0025],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "sent1-32\n",
      " tensor([31,  0,  1, 31, -1,  0])\n",
      "sent1~mask\n",
      " tensor([0, 0, 0, 0, 0, 0])\n",
      "trainable_embedded_sent1:\n",
      " tensor([[-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "out1\n",
      " tensor([[-0.1202,  0.0700,  0.1030,  ...,  0.3267, -0.0606,  0.0025],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727],\n",
      "        [ 0.1073,  0.0089,  0.0006,  ...,  0.0050,  0.1173, -0.0400],\n",
      "        [-0.1202,  0.0700,  0.1030,  ...,  0.3267, -0.0606,  0.0025],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.9479, -1.2583, -0.3336,  ...,  0.8607, -0.1911,  0.6727]],\n",
      "       grad_fn=<PutBackward>)\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "######## test ##############\n",
    "sent1 = torch.tensor([32,1,2,32,0,1])\n",
    "mask = (sent1==1)\n",
    "pretrained_sent1 = sent1.clone()\n",
    "pretrained_sent1[mask] = 0 #assign unk vector to 0\n",
    "fixed_embed = nn.Embedding.from_pretrained(torch.FloatTensor(word_ebd),freeze=True) #freeze those that are not unk\n",
    "trainable_embed = nn.Embedding(1, 300)\n",
    "\n",
    "print('sent1\\n',sent1)\n",
    "\n",
    "\n",
    "\n",
    "print('pretrained_sent1:\\n',pretrained_sent1)\n",
    "out1 = fixed_embed(pretrained_sent1) #embed pretrained \n",
    "print('out1:fixed_embed\\n',out1)\n",
    "\n",
    "sent1 -= 1\n",
    "print('sent1-32\\n',sent1)\n",
    "sent1[~mask] = 0  #把不为32的都标记为0，i.e. mark all that are unk as 1\n",
    "print('sent1~mask\\n',sent1)\n",
    "\n",
    "\n",
    "trainable_embedded_sent1 = trainable_embed(sent1)\n",
    "out1[mask] = trainable_embedded_sent1[mask]\n",
    "print('trainable_embedded_sent1:\\n',trainable_embedded_sent1)\n",
    "print('out1\\n',out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIMS DL",
   "language": "python",
   "name": "aims-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
