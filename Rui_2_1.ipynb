{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Rui_2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rae-Jiang/DSGA1011_NLP/blob/master/Rui_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhzMpzQvDfq-",
        "colab_type": "text"
      },
      "source": [
        "# DS-GA 1011 Homework 2\n",
        "## N-Gram and Neural Language Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQfThHknDB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "096e4180-5807-4082-e258-2d068c23620e"
      },
      "source": [
        "import sys\n",
        "\n",
        "try:\n",
        "    import jsonlines\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install jsonlines\n",
        "  \n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    print('Installing the package, RESTART THIS CELL')\n",
        "    !{sys.executable} -m pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing the package, RESTART THIS CELL\n",
            "Collecting jsonlines\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.12.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsO4H9MIDfq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYhzi9CC3yqJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0ba128b2-c07f-4699-a339-12954c22c380"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "basedir = '/content/drive/My Drive/1011 NLP/HW2/Playground_Rui'\n",
        "data_folder_path = os.path.join(basedir, 'data')\n",
        "model_folder_path = os.path.join(basedir, 'model')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j73WKUf6DfrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_wikitext(filename='wikitext2-sentencized.json'):\n",
        "    if not os.path.exists(filename):\n",
        "        !wget \"https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\" -O $filename\n",
        "    \n",
        "    datasets = json.load(open(filename, 'r'))\n",
        "    for name in datasets:\n",
        "        datasets[name] = [x.split() for x in datasets[name]]\n",
        "    vocab = list(set([t for ts in datasets['train'] for t in ts]))      \n",
        "    print(\"Vocab size: %d\" % (len(vocab)))\n",
        "    return datasets, vocab\n",
        "\n",
        "def perplexity(model, sequences):\n",
        "    n_total = 0\n",
        "    logp_total = 0\n",
        "    for sequence in sequences:\n",
        "        logp_total += model.sequence_logp(sequence)\n",
        "        n_total += len(sequence) + 1  \n",
        "    ppl = 2 ** (- (1.0 / n_total) * logp_total)  \n",
        "    return ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ayhuc3-DfrJ",
        "colab_type": "code",
        "outputId": "3de11a09-e701-4689-c13f-821441453c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "datasets, vocab = load_wikitext()\n",
        "\n",
        "# delta = 0.0005\n",
        "# for n in [2, 3, 4]:\n",
        "#     lm = NGramAdditive(n=n, delta=delta, vsize=len(vocab)+1)  # +1 is for <eos>\n",
        "#     lm.estimate(datasets['train'])\n",
        "\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Train Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['train'])))\n",
        "#     print(\"Baseline (Additive smoothing, n=%d, delta=%.4f)) Valid Perplexity: %.3f\" % (n, delta, perplexity(lm, datasets['valid'])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-05 18:59:32--  https://nyu.box.com/shared/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.box.com (nyu.box.com)... 107.152.26.197, 107.152.27.197\n",
            "Connecting to nyu.box.com (nyu.box.com)|107.152.26.197|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-05 18:59:32--  https://nyu.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Reusing existing connection to nyu.box.com:443.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json [following]\n",
            "--2019-10-05 18:59:33--  https://nyu.app.box.com/public/static/9kb7l7ci30hb6uahhbssjlq0kctr5ii4.json\n",
            "Resolving nyu.app.box.com (nyu.app.box.com)... 107.152.25.199, 107.152.24.199\n",
            "Connecting to nyu.app.box.com (nyu.app.box.com)|107.152.25.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://public.boxcloud.com/d/1/b1!Dmw9_9PxVMVUN2P6bCC12BwhXqVWfjn2OztdaD_-NVrPdCX-ar38KweHD4QQyMa3WiC42Qdb3RdWWSLdonPl6ukn-MJi9nvVgsl2WO2cx8pJmD6IOYpSc2iAXc4z34tEZkTnpHBqnEKIl21tRf-VysXqv8elVxY0kXxN5jdv2q-9MiMKLaIAXJy2rbPaaZX0GiDS5Ry5R_Gigr1ax11kyUrfJZxmvcgh-7nS3Kf72yBaQai35Yg64NbEsWpjdZ6AfducIzKphhKYhF7ekeOWD0CbYIKqYElkMIbKyK_QDr31DIxtMiu_uCbTH2zxO9FFPpXL6mMz1iSe-1xTWu6WfjN_Gnpsy8dlQVcba2O18baSERePz4U7lyP4GOKI-9K2LJpM28F_Ifx-gkHxFCiJv1oMIPac6u9RZsBYTFgg68LIY64CM9onwN5f5k3iH5fdsBmlanG0ft39bd0otqlo7p9Eat7p76f-FemCGhSzz9vZcZHAc_KGbI53oQ_VBJS5uD-DGnu6pcg_BzhTNOJLAWoDnmJdxWSQA-HcCHE-n5NkuHbzTJNFQuRsytwHND4b9S0fHWZVTcFknQ75XpCe2tTFCO0P6LSkuUDsUCby84y8dsC7kzbbtfiZd-7TA7tAU6TVwqIIy4umJmKjJeBobvCBfub8LMEPouaVndNLdsqZasUcXjwgSZRrQ1K9jnNAGQdVfoM-1oqa34oYKxODAMjiAK9xwoUoE6yY5e55nbX05NHMf3mkCJZrXvz6Nejg0IaiJPaJkGjIfl-Q_eCSARsp-0bs44W3lwm8iPtSaXrmbt7vA64mIgkRtHnBoi3Tq9QQbimbKofBi8_MJrSU9lYaQh_vT7OXr-kiUPrGuo8s9C5PzcchXClQsL_hArvAOLLj-wPd35OeumUNthj6Xzp3sAlK-c4IeYnkHfpaumqtEvYyRMBe7UGknXUfDyCyM-WaL9xwXN_YOu_XEk-bpKvzw1CgG7iKwlH0MibqdaZ2rIdytYB7wyVBDaQHPjiQ1L4L41BH26LrTJAfFJhPYVfSgeLK7aY0bx_OeTQRrnoF0K03ywMbabe1N8ANjS2wShd_UKUHwlxbF7oLicEPQ76VlD-gZa-IKmjLo5x1iJ0zFjvS7uWgOoIpiwa7z5jxMyGT2iBVU3sJoqifOliKllEg_9cAcLDmTSemORjQ_EOLrpPu9Y8bRPHnMhVxznbR385a3xjfIYZxCMcW1WuS8PBP4KaLxrTbTH1QFU3OQj9CDxHFZzGZ6ZDX6sIAJefOtwu4gSLC7jNuuS4urhsY6O-a7yDrLR_gCH6io1qUeWInUpTThxxuLajy5gFa2w2J1G-fYcxtu6wDgMRXqjVcoXsvZCbwn1tywmWfECZ-6vfbZOW1PA96Z3nennhFkQM0pG-L0B-B0DGf-g../download [following]\n",
            "--2019-10-05 18:59:33--  https://public.boxcloud.com/d/1/b1!Dmw9_9PxVMVUN2P6bCC12BwhXqVWfjn2OztdaD_-NVrPdCX-ar38KweHD4QQyMa3WiC42Qdb3RdWWSLdonPl6ukn-MJi9nvVgsl2WO2cx8pJmD6IOYpSc2iAXc4z34tEZkTnpHBqnEKIl21tRf-VysXqv8elVxY0kXxN5jdv2q-9MiMKLaIAXJy2rbPaaZX0GiDS5Ry5R_Gigr1ax11kyUrfJZxmvcgh-7nS3Kf72yBaQai35Yg64NbEsWpjdZ6AfducIzKphhKYhF7ekeOWD0CbYIKqYElkMIbKyK_QDr31DIxtMiu_uCbTH2zxO9FFPpXL6mMz1iSe-1xTWu6WfjN_Gnpsy8dlQVcba2O18baSERePz4U7lyP4GOKI-9K2LJpM28F_Ifx-gkHxFCiJv1oMIPac6u9RZsBYTFgg68LIY64CM9onwN5f5k3iH5fdsBmlanG0ft39bd0otqlo7p9Eat7p76f-FemCGhSzz9vZcZHAc_KGbI53oQ_VBJS5uD-DGnu6pcg_BzhTNOJLAWoDnmJdxWSQA-HcCHE-n5NkuHbzTJNFQuRsytwHND4b9S0fHWZVTcFknQ75XpCe2tTFCO0P6LSkuUDsUCby84y8dsC7kzbbtfiZd-7TA7tAU6TVwqIIy4umJmKjJeBobvCBfub8LMEPouaVndNLdsqZasUcXjwgSZRrQ1K9jnNAGQdVfoM-1oqa34oYKxODAMjiAK9xwoUoE6yY5e55nbX05NHMf3mkCJZrXvz6Nejg0IaiJPaJkGjIfl-Q_eCSARsp-0bs44W3lwm8iPtSaXrmbt7vA64mIgkRtHnBoi3Tq9QQbimbKofBi8_MJrSU9lYaQh_vT7OXr-kiUPrGuo8s9C5PzcchXClQsL_hArvAOLLj-wPd35OeumUNthj6Xzp3sAlK-c4IeYnkHfpaumqtEvYyRMBe7UGknXUfDyCyM-WaL9xwXN_YOu_XEk-bpKvzw1CgG7iKwlH0MibqdaZ2rIdytYB7wyVBDaQHPjiQ1L4L41BH26LrTJAfFJhPYVfSgeLK7aY0bx_OeTQRrnoF0K03ywMbabe1N8ANjS2wShd_UKUHwlxbF7oLicEPQ76VlD-gZa-IKmjLo5x1iJ0zFjvS7uWgOoIpiwa7z5jxMyGT2iBVU3sJoqifOliKllEg_9cAcLDmTSemORjQ_EOLrpPu9Y8bRPHnMhVxznbR385a3xjfIYZxCMcW1WuS8PBP4KaLxrTbTH1QFU3OQj9CDxHFZzGZ6ZDX6sIAJefOtwu4gSLC7jNuuS4urhsY6O-a7yDrLR_gCH6io1qUeWInUpTThxxuLajy5gFa2w2J1G-fYcxtu6wDgMRXqjVcoXsvZCbwn1tywmWfECZ-6vfbZOW1PA96Z3nennhFkQM0pG-L0B-B0DGf-g../download\n",
            "Resolving public.boxcloud.com (public.boxcloud.com)... 107.152.25.200\n",
            "Connecting to public.boxcloud.com (public.boxcloud.com)|107.152.25.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12714601 (12M) [application/octet-stream]\n",
            "Saving to: ‘wikitext2-sentencized.json’\n",
            "\n",
            "wikitext2-sentenciz 100%[===================>]  12.12M  25.2MB/s    in 0.5s    \n",
            "\n",
            "2019-10-05 18:59:34 (25.2 MB/s) - ‘wikitext2-sentencized.json’ saved [12714601/12714601]\n",
            "\n",
            "Vocab size: 33175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqIyTFmDfrM",
        "colab_type": "code",
        "outputId": "a6357b67-d503-4c8c-db72-496e7060aa73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datasets.keys()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['train', 'valid', 'test'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o_A_F1yDfrT",
        "colab_type": "text"
      },
      "source": [
        "## II. Neural Language Modeling with a Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7r9Q1aFyDfrW",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "(Hint: you can adopt the `Dictionary`, dataset loading, and training code from the lab for use here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmHuhyOeDfra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, RandomSampler, SequentialSampler, DataLoader\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self, datasets, include_valid=False):\n",
        "        self.tokens = []\n",
        "        self.ids = {}\n",
        "        self.counts = {}\n",
        "        \n",
        "        # add special tokens\n",
        "        self.add_token('<bos>')\n",
        "        self.add_token('<eos>')\n",
        "        self.add_token('<pad>')\n",
        "        self.add_token('<unk>')\n",
        "        \n",
        "        for line in tqdm(datasets['train']):  #显示进度条Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
        "            for w in line:\n",
        "                self.add_token(w)\n",
        "                    \n",
        "        if include_valid is True:\n",
        "            for line in tqdm(datasets['valid']):\n",
        "                for w in line:\n",
        "                    self.add_token(w)\n",
        "                            \n",
        "    def add_token(self, w):\n",
        "        if w not in self.tokens:\n",
        "            self.tokens.append(w)\n",
        "            _w_id = len(self.tokens) - 1\n",
        "            self.ids[w] = _w_id\n",
        "            self.counts[w] = 1\n",
        "        else:\n",
        "            self.counts[w] += 1\n",
        "\n",
        "    def get_id(self, w):\n",
        "        return self.ids[w]\n",
        "    \n",
        "    def get_token(self, idx):\n",
        "        return self.tokens[idx]\n",
        "    \n",
        "    def decode_idx_seq(self, l):\n",
        "        return [self.tokens[i] for i in l]\n",
        "    \n",
        "    def encode_token_seq(self, l):\n",
        "        return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "def tokenize_dataset(datasets, dictionary, ngram_order=2):\n",
        "    tokenized_datasets = {}\n",
        "    for split, dataset in datasets.items():\n",
        "        _current_dictified = []\n",
        "        for l in tqdm(dataset):\n",
        "            l = ['<bos>']*(ngram_order-1) + l + ['<eos>']\n",
        "            encoded_l = dictionary.encode_token_seq(l)\n",
        "            _current_dictified.append(encoded_l)\n",
        "        tokenized_datasets[split] = _current_dictified\n",
        "        \n",
        "    return tokenized_datasets\n",
        "\n",
        "class TensoredDataset(Dataset):\n",
        "    def __init__(self, list_of_lists_of_tokens):\n",
        "        self.input_tensors = []\n",
        "        self.target_tensors = []\n",
        "        \n",
        "        for sample in list_of_lists_of_tokens:\n",
        "            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n",
        "            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))#从第二个词开始就是target，shifted version of sequence as  \n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_tensors)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # return a (input, target) tuple\n",
        "        return (self.input_tensors[idx], self.target_tensors[idx])\n",
        "    \n",
        "def pad_list_of_tensors(list_of_tensors, pad_token):\n",
        "    max_length = max([t.size(-1) for t in list_of_tensors])\n",
        "    padded_list = []\n",
        "    \n",
        "    for t in list_of_tensors:\n",
        "        padded_tensor = torch.cat([t, torch.tensor([[pad_token]*(max_length - t.size(-1))], dtype=torch.long)], dim = -1)\n",
        "        padded_list.append(padded_tensor)\n",
        "        \n",
        "    padded_tensor = torch.cat(padded_list, dim=0)\n",
        "    \n",
        "    return padded_tensor\n",
        "\n",
        "def pad_collate_fn(batch):\n",
        "    # batch is a list of sample tuples\n",
        "    input_list = [s[0] for s in batch]\n",
        "    target_list = [s[1] for s in batch]\n",
        "    \n",
        "    pad_token = wiki_dict.get_id('<pad>') # pad_token = 2\n",
        "    \n",
        "    input_tensor = pad_list_of_tensors(input_list, pad_token)\n",
        "    target_tensor = pad_list_of_tensors(target_list, pad_token)\n",
        "    \n",
        "    return input_tensor, target_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceQhyp7tDfrc",
        "colab_type": "code",
        "outputId": "15392117-664c-4dab-ba27-e83a9e874150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "wiki_dict = Dictionary(datasets,include_valid=True)\n",
        "# checking some example\n",
        "print(' '.join(datasets['train'][3010]))\n",
        "encoded = wiki_dict.encode_token_seq(datasets['train'][3010])\n",
        "print(f'\\n encoded - {encoded}')\n",
        "decoded = wiki_dict.decode_idx_seq(encoded)\n",
        "print(f'\\n decoded - {decoded}')\n",
        "\n",
        "tokenized_datasets = tokenize_dataset(datasets, wiki_dict)\n",
        "tensor_dataset = {}\n",
        "\n",
        "for split, listoflists in tokenized_datasets.items():\n",
        "    tensor_dataset[split] = TensoredDataset(listoflists)\n",
        "    \n",
        "# check the first example\n",
        "tensor_dataset['train'][0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [02:11<00:00, 595.25it/s]\n",
            "100%|██████████| 8464/8464 [00:10<00:00, 835.94it/s]\n",
            " 18%|█▊        | 13943/78274 [00:00<00:00, 139427.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The Nataraja and Ardhanarishvara sculptures are also attributed to the Rashtrakutas .\n",
            "\n",
            " encoded - [75, 8816, 30, 8817, 8732, 70, 91, 2960, 13, 6, 8806, 39]\n",
            "\n",
            " decoded - ['The', 'Nataraja', 'and', 'Ardhanarishvara', 'sculptures', 'are', 'also', 'attributed', 'to', 'the', 'Rashtrakutas', '.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 78274/78274 [00:00<00:00, 99513.94it/s]\n",
            "100%|██████████| 8464/8464 [00:00<00:00, 124796.86it/s]\n",
            "100%|██████████| 9708/9708 [00:00<00:00, 123327.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10,\n",
              "          19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]]),\n",
              " tensor([[ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  4, 15, 16, 17, 18, 10, 19,\n",
              "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,  1]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Q_JFcWDfrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders = {}\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "for split, wiki_dataset in tensor_dataset.items():\n",
        "    loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZfDCOxgDfrZ",
        "colab_type": "text"
      },
      "source": [
        "### II.1 LSTM and Hyper-Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7esYzeIQDfrh",
        "colab_type": "text"
      },
      "source": [
        "#### make model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02hmRa9u9uBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.RNN (Baseline)\n",
        "class RNN_LM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, lstm and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.rnn = nn.RNN(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['dropout'], batch_first=True) #inputs,outputs\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        rnn_output,hidden = self.rnn(embeddings) #return a tuple,out的last slice和hidden一样（因为out give you access to all hidden states in the sequence, hidden will allow you to continue the sequence and backpropagate)        \n",
        "        logits = self.projection(rnn_output)#why 0?rnn gives all outputs of all hiddens it has computed so far.\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL6g8qkWDfri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2.LSTM\n",
        "class LSTM_LM(nn.Module):\n",
        "    \"\"\"\n",
        "    This model combines embedding, lstm and projection layer into a single model\n",
        "    \"\"\"\n",
        "    def __init__(self, options):\n",
        "        super().__init__()\n",
        "        \n",
        "        # create each LM part here \n",
        "        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n",
        "        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['dropout'], batch_first=True) #inputs,outputs\n",
        "        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n",
        "        \n",
        "    def forward(self, encoded_input_sequence):\n",
        "        \"\"\"\n",
        "        Forward method process the input from token ids to logits\n",
        "        \"\"\"\n",
        "        embeddings = self.lookup(encoded_input_sequence)\n",
        "        lstm_output,hidden = self.lstm(embeddings) #out的last slice和hidden一样（因为out give you access to all hidden states in the sequence, hidden will allow you to continue the sequence and backpropagate)\n",
        "        logits = self.projection(lstm_output)\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJFaVPJmjlMS",
        "colab_type": "text"
      },
      "source": [
        "##### RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KUQspuhDfro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating a model, criterion and optimizer\n",
        "#specify which model(RNN or LSTM) or if use a pretrained or not\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "assert current_device == 'cuda'\n",
        "\n",
        "load_pretrained = False\n",
        "model_type = 'rnn'\n",
        "\n",
        "if load_pretrained:\n",
        "    if model_type == 'lstm':\n",
        "        if not os.path.exists('wiki_lstm_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_lstm_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "        \n",
        "    if model_type =='rnn':\n",
        "        if not os.path.exists('wiki_rnn_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_rnn_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "else:\n",
        "    embedding_dim = 64\n",
        "    hidden_size = 128\n",
        "    num_layers = 2\n",
        "    dropout = 0.1\n",
        "\n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout': dropout,\n",
        "    }\n",
        "    if model_type == 'lstm':\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "    if model_type == 'rnn':\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'),reduction='sum')\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeB49BNTDfrq",
        "colab_type": "code",
        "outputId": "8740bbd4-05f4-48e9-b7bf-156d98f520f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_LM(\n",
              "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
              "  (rnn): RNN(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7de25b97-9e70-4716-e7b0-ed1e61ee95d0",
        "id": "O0_s1CYkGgu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(10):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        \n",
        "        train_loss_cache = 0\n",
        "        train_non_pad_tokens_cache = 0\n",
        "        \n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            train_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            train_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "            loss /= non_pad_tokens  # very important to normalize your current loss before you run .backward()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                        \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "            \n",
        "    #do valid\n",
        "\n",
        "    valid_loss_cache = 0\n",
        "    valid_non_pad_tokens_cache = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            valid_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "        avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
        "            \n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "if model_type == 'lstm':\n",
        "    lstm_plot_cache = plot_cache\n",
        "if model_type == 'rnn':\n",
        "    rnn_plot_cache = plot_cache"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4288\n",
            "Step 100 avg train loss = 7.7435\n",
            "Step 200 avg train loss = 7.3367\n",
            "Step 300 avg train loss = 7.1327\n",
            "Step 400 avg train loss = 7.0006\n",
            "Step 500 avg train loss = 6.8983\n",
            "Step 600 avg train loss = 6.8192\n",
            "Step 700 avg train loss = 6.7519\n",
            "Step 800 avg train loss = 6.6939\n",
            "Step 900 avg train loss = 6.6447\n",
            "Step 1000 avg train loss = 6.6010\n",
            "Step 1100 avg train loss = 6.5596\n",
            "Step 1200 avg train loss = 6.5238\n",
            "Step 1300 avg train loss = 6.4894\n",
            "Step 1400 avg train loss = 6.4585\n",
            "Step 1500 avg train loss = 6.4313\n",
            "Step 1600 avg train loss = 6.4043\n",
            "Step 1700 avg train loss = 6.3803\n",
            "Step 1800 avg train loss = 6.3568\n",
            "Step 1900 avg train loss = 6.3340\n",
            "Step 2000 avg train loss = 6.3138\n",
            "Step 2100 avg train loss = 6.2937\n",
            "Step 2200 avg train loss = 6.2757\n",
            "Step 2300 avg train loss = 6.2585\n",
            "Step 2400 avg train loss = 6.2411\n",
            "Validation loss after 0 epoch = 5.6907\n",
            "Step 0 avg train loss = 5.5617\n",
            "Step 100 avg train loss = 5.7252\n",
            "Step 200 avg train loss = 5.7123\n",
            "Step 300 avg train loss = 5.7101\n",
            "Step 400 avg train loss = 5.7114\n",
            "Step 500 avg train loss = 5.7071\n",
            "Step 600 avg train loss = 5.7050\n",
            "Step 700 avg train loss = 5.6985\n",
            "Step 800 avg train loss = 5.6952\n",
            "Step 900 avg train loss = 5.6936\n",
            "Step 1000 avg train loss = 5.6903\n",
            "Step 1100 avg train loss = 5.6854\n",
            "Step 1200 avg train loss = 5.6822\n",
            "Step 1300 avg train loss = 5.6798\n",
            "Step 1400 avg train loss = 5.6766\n",
            "Step 1500 avg train loss = 5.6743\n",
            "Step 1600 avg train loss = 5.6716\n",
            "Step 1700 avg train loss = 5.6666\n",
            "Step 1800 avg train loss = 5.6608\n",
            "Step 1900 avg train loss = 5.6568\n",
            "Step 2000 avg train loss = 5.6536\n",
            "Step 2100 avg train loss = 5.6490\n",
            "Step 2200 avg train loss = 5.6460\n",
            "Step 2300 avg train loss = 5.6430\n",
            "Step 2400 avg train loss = 5.6389\n",
            "Validation loss after 1 epoch = 5.5088\n",
            "Step 0 avg train loss = 5.2260\n",
            "Step 100 avg train loss = 5.3873\n",
            "Step 200 avg train loss = 5.3914\n",
            "Step 300 avg train loss = 5.3962\n",
            "Step 400 avg train loss = 5.3966\n",
            "Step 500 avg train loss = 5.4003\n",
            "Step 600 avg train loss = 5.4021\n",
            "Step 700 avg train loss = 5.4051\n",
            "Step 800 avg train loss = 5.4045\n",
            "Step 900 avg train loss = 5.4045\n",
            "Step 1000 avg train loss = 5.4032\n",
            "Step 1100 avg train loss = 5.4032\n",
            "Step 1200 avg train loss = 5.4018\n",
            "Step 1300 avg train loss = 5.4021\n",
            "Step 1400 avg train loss = 5.4038\n",
            "Step 1500 avg train loss = 5.4022\n",
            "Step 1600 avg train loss = 5.4029\n",
            "Step 1700 avg train loss = 5.4016\n",
            "Step 1800 avg train loss = 5.4017\n",
            "Step 1900 avg train loss = 5.4002\n",
            "Step 2000 avg train loss = 5.3986\n",
            "Step 2100 avg train loss = 5.3973\n",
            "Step 2200 avg train loss = 5.3964\n",
            "Step 2300 avg train loss = 5.3957\n",
            "Step 2400 avg train loss = 5.3955\n",
            "Validation loss after 2 epoch = 5.4235\n",
            "Step 0 avg train loss = 5.3070\n",
            "Step 100 avg train loss = 5.2171\n",
            "Step 200 avg train loss = 5.2107\n",
            "Step 300 avg train loss = 5.2153\n",
            "Step 400 avg train loss = 5.2164\n",
            "Step 500 avg train loss = 5.2230\n",
            "Step 600 avg train loss = 5.2229\n",
            "Step 700 avg train loss = 5.2198\n",
            "Step 800 avg train loss = 5.2236\n",
            "Step 900 avg train loss = 5.2222\n",
            "Step 1000 avg train loss = 5.2233\n",
            "Step 1100 avg train loss = 5.2246\n",
            "Step 1200 avg train loss = 5.2272\n",
            "Step 1300 avg train loss = 5.2286\n",
            "Step 1400 avg train loss = 5.2291\n",
            "Step 1500 avg train loss = 5.2299\n",
            "Step 1600 avg train loss = 5.2301\n",
            "Step 1700 avg train loss = 5.2307\n",
            "Step 1800 avg train loss = 5.2301\n",
            "Step 1900 avg train loss = 5.2310\n",
            "Step 2000 avg train loss = 5.2319\n",
            "Step 2100 avg train loss = 5.2329\n",
            "Step 2200 avg train loss = 5.2323\n",
            "Step 2300 avg train loss = 5.2326\n",
            "Step 2400 avg train loss = 5.2317\n",
            "Validation loss after 3 epoch = 5.3817\n",
            "Step 0 avg train loss = 5.1949\n",
            "Step 100 avg train loss = 5.0591\n",
            "Step 200 avg train loss = 5.0743\n",
            "Step 300 avg train loss = 5.0767\n",
            "Step 400 avg train loss = 5.0814\n",
            "Step 500 avg train loss = 5.0910\n",
            "Step 600 avg train loss = 5.0940\n",
            "Step 700 avg train loss = 5.0953\n",
            "Step 800 avg train loss = 5.0944\n",
            "Step 900 avg train loss = 5.0976\n",
            "Step 1000 avg train loss = 5.0988\n",
            "Step 1100 avg train loss = 5.1006\n",
            "Step 1200 avg train loss = 5.1016\n",
            "Step 1300 avg train loss = 5.1031\n",
            "Step 1400 avg train loss = 5.1051\n",
            "Step 1500 avg train loss = 5.1057\n",
            "Step 1600 avg train loss = 5.1070\n",
            "Step 1700 avg train loss = 5.1073\n",
            "Step 1800 avg train loss = 5.1089\n",
            "Step 1900 avg train loss = 5.1091\n",
            "Step 2000 avg train loss = 5.1096\n",
            "Step 2100 avg train loss = 5.1095\n",
            "Step 2200 avg train loss = 5.1116\n",
            "Step 2300 avg train loss = 5.1119\n",
            "Step 2400 avg train loss = 5.1124\n",
            "Validation loss after 4 epoch = 5.3536\n",
            "Step 0 avg train loss = 5.1118\n",
            "Step 100 avg train loss = 4.9606\n",
            "Step 200 avg train loss = 4.9744\n",
            "Step 300 avg train loss = 4.9781\n",
            "Step 400 avg train loss = 4.9826\n",
            "Step 500 avg train loss = 4.9829\n",
            "Step 600 avg train loss = 4.9834\n",
            "Step 700 avg train loss = 4.9875\n",
            "Step 800 avg train loss = 4.9902\n",
            "Step 900 avg train loss = 4.9921\n",
            "Step 1000 avg train loss = 4.9919\n",
            "Step 1100 avg train loss = 4.9930\n",
            "Step 1200 avg train loss = 4.9963\n",
            "Step 1300 avg train loss = 4.9989\n",
            "Step 1400 avg train loss = 5.0014\n",
            "Step 1500 avg train loss = 5.0027\n",
            "Step 1600 avg train loss = 5.0038\n",
            "Step 1700 avg train loss = 5.0055\n",
            "Step 1800 avg train loss = 5.0080\n",
            "Step 1900 avg train loss = 5.0096\n",
            "Step 2000 avg train loss = 5.0096\n",
            "Step 2100 avg train loss = 5.0115\n",
            "Step 2200 avg train loss = 5.0139\n",
            "Step 2300 avg train loss = 5.0151\n",
            "Step 2400 avg train loss = 5.0164\n",
            "Validation loss after 5 epoch = 5.3390\n",
            "Step 0 avg train loss = 4.6192\n",
            "Step 100 avg train loss = 4.8753\n",
            "Step 200 avg train loss = 4.8806\n",
            "Step 300 avg train loss = 4.8826\n",
            "Step 400 avg train loss = 4.8908\n",
            "Step 500 avg train loss = 4.8952\n",
            "Step 600 avg train loss = 4.8975\n",
            "Step 700 avg train loss = 4.9010\n",
            "Step 800 avg train loss = 4.9045\n",
            "Step 900 avg train loss = 4.9082\n",
            "Step 1000 avg train loss = 4.9084\n",
            "Step 1100 avg train loss = 4.9104\n",
            "Step 1200 avg train loss = 4.9135\n",
            "Step 1300 avg train loss = 4.9148\n",
            "Step 1400 avg train loss = 4.9185\n",
            "Step 1500 avg train loss = 4.9204\n",
            "Step 1600 avg train loss = 4.9213\n",
            "Step 1700 avg train loss = 4.9223\n",
            "Step 1800 avg train loss = 4.9232\n",
            "Step 1900 avg train loss = 4.9249\n",
            "Step 2000 avg train loss = 4.9271\n",
            "Step 2100 avg train loss = 4.9283\n",
            "Step 2200 avg train loss = 4.9304\n",
            "Step 2300 avg train loss = 4.9330\n",
            "Step 2400 avg train loss = 4.9356\n",
            "Validation loss after 6 epoch = 5.3320\n",
            "Step 0 avg train loss = 4.8354\n",
            "Step 100 avg train loss = 4.8200\n",
            "Step 200 avg train loss = 4.8152\n",
            "Step 300 avg train loss = 4.8197\n",
            "Step 400 avg train loss = 4.8274\n",
            "Step 500 avg train loss = 4.8300\n",
            "Step 600 avg train loss = 4.8343\n",
            "Step 700 avg train loss = 4.8388\n",
            "Step 800 avg train loss = 4.8408\n",
            "Step 900 avg train loss = 4.8410\n",
            "Step 1000 avg train loss = 4.8426\n",
            "Step 1100 avg train loss = 4.8463\n",
            "Step 1200 avg train loss = 4.8494\n",
            "Step 1300 avg train loss = 4.8520\n",
            "Step 1400 avg train loss = 4.8527\n",
            "Step 1500 avg train loss = 4.8524\n",
            "Step 1600 avg train loss = 4.8544\n",
            "Step 1700 avg train loss = 4.8562\n",
            "Step 1800 avg train loss = 4.8586\n",
            "Step 1900 avg train loss = 4.8610\n",
            "Step 2000 avg train loss = 4.8638\n",
            "Step 2100 avg train loss = 4.8646\n",
            "Step 2200 avg train loss = 4.8665\n",
            "Step 2300 avg train loss = 4.8682\n",
            "Step 2400 avg train loss = 4.8697\n",
            "Validation loss after 7 epoch = 5.3329\n",
            "Step 0 avg train loss = 4.6443\n",
            "Step 100 avg train loss = 4.7217\n",
            "Step 200 avg train loss = 4.7359\n",
            "Step 300 avg train loss = 4.7474\n",
            "Step 400 avg train loss = 4.7591\n",
            "Step 500 avg train loss = 4.7710\n",
            "Step 600 avg train loss = 4.7733\n",
            "Step 700 avg train loss = 4.7784\n",
            "Step 800 avg train loss = 4.7813\n",
            "Step 900 avg train loss = 4.7821\n",
            "Step 1000 avg train loss = 4.7849\n",
            "Step 1100 avg train loss = 4.7885\n",
            "Step 1200 avg train loss = 4.7902\n",
            "Step 1300 avg train loss = 4.7916\n",
            "Step 1400 avg train loss = 4.7948\n",
            "Step 1500 avg train loss = 4.7992\n",
            "Step 1600 avg train loss = 4.8019\n",
            "Step 1700 avg train loss = 4.8037\n",
            "Step 1800 avg train loss = 4.8067\n",
            "Step 1900 avg train loss = 4.8092\n",
            "Step 2000 avg train loss = 4.8105\n",
            "Step 2100 avg train loss = 4.8122\n",
            "Step 2200 avg train loss = 4.8144\n",
            "Step 2300 avg train loss = 4.8157\n",
            "Step 2400 avg train loss = 4.8172\n",
            "Validation loss after 8 epoch = 5.3305\n",
            "Step 0 avg train loss = 4.6659\n",
            "Step 100 avg train loss = 4.6676\n",
            "Step 200 avg train loss = 4.6970\n",
            "Step 300 avg train loss = 4.6956\n",
            "Step 400 avg train loss = 4.7083\n",
            "Step 500 avg train loss = 4.7159\n",
            "Step 600 avg train loss = 4.7188\n",
            "Step 700 avg train loss = 4.7210\n",
            "Step 800 avg train loss = 4.7234\n",
            "Step 900 avg train loss = 4.7267\n",
            "Step 1000 avg train loss = 4.7286\n",
            "Step 1100 avg train loss = 4.7333\n",
            "Step 1200 avg train loss = 4.7367\n",
            "Step 1300 avg train loss = 4.7387\n",
            "Step 1400 avg train loss = 4.7405\n",
            "Step 1500 avg train loss = 4.7435\n",
            "Step 1600 avg train loss = 4.7455\n",
            "Step 1700 avg train loss = 4.7470\n",
            "Step 1800 avg train loss = 4.7490\n",
            "Step 1900 avg train loss = 4.7509\n",
            "Step 2000 avg train loss = 4.7526\n",
            "Step 2100 avg train loss = 4.7555\n",
            "Step 2200 avg train loss = 4.7581\n",
            "Step 2300 avg train loss = 4.7608\n",
            "Step 2400 avg train loss = 4.7630\n",
            "Validation loss after 9 epoch = 5.3316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBj_zAnoeQJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model, like pickle dump\n",
        "model_path = os.path.join(model_folder_path,'hw2_2.1.1_rnn_lm.pt')\n",
        "if not load_pretrained:\n",
        "    torch.save({\n",
        "        'options': options,\n",
        "        'loss_cache': plot_cache,\n",
        "        'model_dict': model.state_dict() #all params\n",
        "    }, model_path) #file name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul-adwOIaTJ4",
        "colab_type": "text"
      },
      "source": [
        " ##### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYnnYJnmFnSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_pretrained = False\n",
        "model_type = 'lstm'\n",
        "# creating a model, criterion and optimizer\n",
        "#specify which model(RNN or LSTM) or if use a pretrained or not\n",
        "num_gpus = torch.cuda.device_count()\n",
        "if num_gpus > 0:\n",
        "    current_device = 'cuda'\n",
        "else:\n",
        "    current_device = 'cpu'\n",
        "assert current_device == 'cuda'\n",
        "\n",
        "if load_pretrained:\n",
        "    if model_type == 'lstm':\n",
        "        if not os.path.exists('wiki_lstm_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_lstm_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "        \n",
        "    if model_type =='rnn':\n",
        "        if not os.path.exists('wiki_rnn_lm.pt'):\n",
        "            raise EOFError('No model downloaded!')\n",
        "        model_dict = torch.load('wiki_rnn_lm.pt')\n",
        "\n",
        "        options = model_dict['options']\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "        model.load_state_dict(model_dict['model_dict'])\n",
        "\n",
        "else:\n",
        "    embedding_dim = 64\n",
        "    hidden_size = 128\n",
        "    num_layers = 2\n",
        "    dropout = 0.1\n",
        "\n",
        "    options = {\n",
        "        'num_embeddings': len(wiki_dict),\n",
        "        'embedding_dim': embedding_dim,\n",
        "        'padding_idx': wiki_dict.get_id('<pad>'),\n",
        "        'input_size': embedding_dim,\n",
        "        'hidden_size': hidden_size,\n",
        "        'num_layers': num_layers,\n",
        "        'dropout': dropout,\n",
        "    }\n",
        "    if model_type == 'lstm':\n",
        "        model = LSTM_LM(options).to(current_device)\n",
        "    if model_type == 'rnn':\n",
        "        model = RNN_LM(options).to(current_device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=wiki_dict.get_id('<pad>'),reduction='sum')\n",
        "model_parameters = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.Adam(model_parameters, lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmpVbKLVZ_vv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "67f619f4-571a-4f43-b809-6cc2e5551b01"
      },
      "source": [
        "model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_LM(\n",
              "  (lookup): Embedding(33181, 64, padding_idx=2)\n",
              "  (lstm): LSTM(64, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
              "  (projection): Linear(in_features=128, out_features=33181, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma9bDa_caPdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "51be537c-3e1a-428e-d574-ae10530a2e1b"
      },
      "source": [
        "# now we make same training loop, now with dataset and the model\n",
        "\n",
        "plot_cache = []\n",
        "\n",
        "for epoch_number in range(10):\n",
        "    avg_loss=0\n",
        "    if not load_pretrained:\n",
        "        # do train\n",
        "        model.train()\n",
        "        \n",
        "        train_loss_cache = 0\n",
        "        train_non_pad_tokens_cache = 0\n",
        "        \n",
        "        for i, (inp, target) in enumerate(loaders['train']):\n",
        "            optimizer.zero_grad()\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "            \n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            train_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            train_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "            loss /= non_pad_tokens  # very important to normalize your current loss before you run .backward()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "                        \n",
        "            if i % 100 == 0:\n",
        "                avg_loss = train_loss_cache / train_non_pad_tokens_cache\n",
        "                print('Step {} avg train loss = {:.{prec}f}'.format(i, avg_loss, prec=4))\n",
        "            \n",
        "    #do valid\n",
        "\n",
        "    valid_loss_cache = 0\n",
        "    valid_non_pad_tokens_cache = 0\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (inp, target) in enumerate(loaders['valid']):\n",
        "            inp = inp.to(current_device)\n",
        "            target = target.to(current_device)\n",
        "            logits = model(inp)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n",
        "            valid_loss_cache += loss.item()  # still sum here\n",
        "            \n",
        "            ### HERE WE COMPUTE NUMBER OF NON_PAD TOKENS IN THE TARGET\n",
        "            non_pad_tokens = target.view(-1).ne(wiki_dict.get_id('<pad>')).sum().item()\n",
        "            \n",
        "            valid_non_pad_tokens_cache += non_pad_tokens\n",
        "            \n",
        "        avg_val_loss = valid_loss_cache / valid_non_pad_tokens_cache\n",
        "            \n",
        "        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number, avg_val_loss, prec=4))\n",
        "        \n",
        "    plot_cache.append((avg_loss, avg_val_loss))\n",
        "\n",
        "    if load_pretrained:\n",
        "        break\n",
        "if model_type == 'lstm':\n",
        "    lstm_plot_cache = plot_cache\n",
        "if model_type == 'rnn':\n",
        "    rnn_plot_cache = plot_cache"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 avg train loss = 10.4115\n",
            "Step 100 avg train loss = 7.8970\n",
            "Step 200 avg train loss = 7.5331\n",
            "Step 300 avg train loss = 7.3797\n",
            "Step 400 avg train loss = 7.2787\n",
            "Step 500 avg train loss = 7.1952\n",
            "Step 600 avg train loss = 7.1188\n",
            "Step 700 avg train loss = 7.0548\n",
            "Step 800 avg train loss = 6.9977\n",
            "Step 900 avg train loss = 6.9464\n",
            "Step 1000 avg train loss = 6.9006\n",
            "Step 1100 avg train loss = 6.8604\n",
            "Step 1200 avg train loss = 6.8219\n",
            "Step 1300 avg train loss = 6.7868\n",
            "Step 1400 avg train loss = 6.7548\n",
            "Step 1500 avg train loss = 6.7245\n",
            "Step 1600 avg train loss = 6.6953\n",
            "Step 1700 avg train loss = 6.6677\n",
            "Step 1800 avg train loss = 6.6415\n",
            "Step 1900 avg train loss = 6.6173\n",
            "Step 2000 avg train loss = 6.5938\n",
            "Step 2100 avg train loss = 6.5720\n",
            "Step 2200 avg train loss = 6.5504\n",
            "Step 2300 avg train loss = 6.5297\n",
            "Step 2400 avg train loss = 6.5102\n",
            "Validation loss after 0 epoch = 5.8626\n",
            "Step 0 avg train loss = 6.0764\n",
            "Step 100 avg train loss = 5.9653\n",
            "Step 200 avg train loss = 5.9570\n",
            "Step 300 avg train loss = 5.9474\n",
            "Step 400 avg train loss = 5.9396\n",
            "Step 500 avg train loss = 5.9293\n",
            "Step 600 avg train loss = 5.9202\n",
            "Step 700 avg train loss = 5.9143\n",
            "Step 800 avg train loss = 5.9084\n",
            "Step 900 avg train loss = 5.9018\n",
            "Step 1000 avg train loss = 5.8988\n",
            "Step 1100 avg train loss = 5.8910\n",
            "Step 1200 avg train loss = 5.8852\n",
            "Step 1300 avg train loss = 5.8790\n",
            "Step 1400 avg train loss = 5.8721\n",
            "Step 1500 avg train loss = 5.8659\n",
            "Step 1600 avg train loss = 5.8605\n",
            "Step 1700 avg train loss = 5.8537\n",
            "Step 1800 avg train loss = 5.8479\n",
            "Step 1900 avg train loss = 5.8411\n",
            "Step 2000 avg train loss = 5.8342\n",
            "Step 2100 avg train loss = 5.8285\n",
            "Step 2200 avg train loss = 5.8221\n",
            "Step 2300 avg train loss = 5.8157\n",
            "Step 2400 avg train loss = 5.8095\n",
            "Validation loss after 1 epoch = 5.5706\n",
            "Step 0 avg train loss = 5.8510\n",
            "Step 100 avg train loss = 5.5757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpC7irn10bEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving the model, like pickle dump\n",
        "model_path = os.path.join(model_folder_path,'hw2_2.1.1_lstm_lm.pt')\n",
        "if not load_pretrained:\n",
        "    torch.save({\n",
        "        'options': options,\n",
        "        'loss_cache': plot_cache,\n",
        "        'model_dict': model.state_dict() #all params\n",
        "    }, model_path) #file name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb2JEbttDfrt",
        "colab_type": "text"
      },
      "source": [
        "#### Results (LSTM vs. Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piSCUkz6Dfru",
        "colab_type": "code",
        "outputId": "a3e4e2db-d777-4e64-8ca1-8e0f05eecaa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [i[0] for i in lstm_plot_cache], label='LSTM Train loss')\n",
        "plt.plot(epochs, [i[1] for i in lstm_plot_cache], label='LSTM Valid loss')\n",
        "plt.plot(epochs, [i[0] for i in rnn_plot_cache], label='RNN Train loss')\n",
        "plt.plot(epochs, [i[1] for i in rnn_plot_cache], label='RNN Valid loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Loss curves of RNN/LSTM')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsnA3xhgfZOFujDE+6P8DcYWxoElVjIsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXt5DYN0OuM",
        "colab_type": "code",
        "outputId": "4d9665a7-c12c-4afb-9da6-6eb001464009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "epochs = numpy.array(list(range(len(plot_cache))))\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in lstm_plot_cache], label='LSTM Train ppl')\n",
        "plt.plot(epochs, [2**(i[1]/numpy.log(2)) for i in lstm_plot_cache], label='LSTM Valid ppl')\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in rnn_plot_cache], label='RNN Train ppl')\n",
        "plt.plot(epochs, [2**(i[0]/numpy.log(2)) for i in rnn_plot_cache], label='RNN Train ppl')\n",
        "         \n",
        "plt.legend()\n",
        "plt.title('PPL curves of RNN/LSTM')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkcDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyoTgq1L5XNB",
        "colab_type": "code",
        "outputId": "19d6e312-5364-423b-9534-15e14fd19a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('LSTM PPL:',2**(lstm_plot_cache[-1][1]/numpy.log(2)),'RNN PPL',2**(rnn_plot_cache[-1][1]/numpy.log(2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192.74115137346396"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCryReIvDfrx",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Variation Based on Hyperparameter Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zjxZThSDfry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QsVFpSyDfr0",
        "colab_type": "text"
      },
      "source": [
        "### II.2 Learned Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BG-64bZDfr1",
        "colab_type": "text"
      },
      "source": [
        "#### Utilities\n",
        "\n",
        "Below is code to use [UMAP](https://umap-learn.readthedocs.io/en/latest/) to find a 2-dimensional representation of a weight matrix, and plot the resulting 2-dimensional points that correspond to certain words.\n",
        "\n",
        "Use `!pip install umap-learn` to install UMAP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRNFbTpbDfr1",
        "colab_type": "code",
        "outputId": "fffa715e-c30b-42b6-e99a-1e48f283ba74",
        "colab": {}
      },
      "source": [
        "%pylab inline \n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def umap_plot(weight_matrix, word_ids, words):\n",
        "    \"\"\"Run UMAP on the entire Vxd `weight_matrix` (e.g. model.lookup.weight or model.projection.weight),\n",
        "    And plot the points corresponding to the given `word_ids`. \"\"\"\n",
        "    reduced = umap.UMAP(min_dist=0.0001).fit_transform(weight_matrix.detach().cpu().numpy())\n",
        "    plt.figure(figsize=(20,20))\n",
        "\n",
        "    to_plot = reduced[word_ids, :]\n",
        "    plt.scatter(to_plot[:, 0], to_plot[:, 1])\n",
        "    for i, word_id in enumerate(word_ids):\n",
        "        current_point = to_plot[i]\n",
        "        plt.annotate(words[i], (current_point[0], current_point[1]))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5pA6188Dfr4",
        "colab_type": "code",
        "outputId": "c0b05ed6-4daa-4a30-8d41-0c9da2627616",
        "colab": {}
      },
      "source": [
        "Vsize = 100                                 # e.g. len(dictionary)\n",
        "d = 32                                      # e.g. model.lookup.weight.size(1) \n",
        "fake_weight_matrix = torch.randn(Vsize, d)  # e.g. model.lookup.weight\n",
        "\n",
        "words = ['the', 'dog', 'ran']\n",
        "word_ids = [4, 54, 20]                  # e.g. use dictionary.get_id on a list of words\n",
        "\n",
        "umap_plot(fake_weight_matrix, word_ids, words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy51Ty42Dfr6",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.1 Word Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q27P_vADfr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK0vwMRODfr8",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.2 Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D9sXOB-Dfr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdcmGhoGDfr_",
        "colab_type": "text"
      },
      "source": [
        "#### II.2.3 Projection Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdJ0N2ZLDfsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4CvOpFiDfsC",
        "colab_type": "text"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOkUcPUQDfsD",
        "colab_type": "text"
      },
      "source": [
        "### II.3 Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zqqh9zOFDfsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4A70fbPDfsF",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.2 Highest and Lowest scoring sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKVZeX7fDfsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdSjiqSuDfsI",
        "colab_type": "text"
      },
      "source": [
        "#### II.3.3 Modified sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIrTRqXIDfsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdC8mh-8DfsL",
        "colab_type": "text"
      },
      "source": [
        "### II.4 Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muPOk7bpDfsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IN725uaDfsN",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.3 Number of unique tokens and sequence length \n",
        "\n",
        "(1,000 samples vs. 1,000 randomly selected validation-set sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3bzznPvDfsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYA_HRqXDfsQ",
        "colab_type": "text"
      },
      "source": [
        "#### II.4.4 Example Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiuLWtZXDfsQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}